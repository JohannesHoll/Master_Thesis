{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###necessary libaries###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seglearn.transform import FeatureRep, SegmentXYForecast, last\n",
    "from subprocess import check_output\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Flatten\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import newaxis\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "model_seed = 100\n",
    "# ensure same output results\n",
    "seed(101)\n",
    "tf.random.set_seed(model_seed)\n",
    "\n",
    "# file where csv files lies\n",
    "path = r'C:\\Users\\victo\\Master_Thesis\\merging_data\\renault\\hourly\\merged_files'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# read files to pandas frame\n",
    "list_of_files = []\n",
    "\n",
    "for filename in all_files:\n",
    "    list_of_files.append(pd.read_csv(filename,\n",
    "                                     sep=',',\n",
    "                                     )\n",
    "                         )\n",
    "\n",
    "# Concatenate all content of files into one DataFrames\n",
    "concatenate_dataframe = pd.concat(list_of_files,\n",
    "                                  ignore_index=True,\n",
    "                                  axis=0,\n",
    "                                  )\n",
    "\n",
    "# print(concatenate_dataframe)\n",
    "\n",
    "### analysis with flair sentiment content\n",
    "new_df_flair_content = concatenate_dataframe[['Date',\n",
    "                                              'OPEN',\n",
    "                                              'HIGH',\n",
    "                                              'LOW',\n",
    "                                              'CLOSE',\n",
    "                                              'VOLUME',\n",
    "                                              'flair_sentiment_content_score']]\n",
    "\n",
    "new_df_flair_content['flair_sentiment_content_score'] = new_df_flair_content['flair_sentiment_content_score'].fillna(0)\n",
    "# new_df_flair_content[['Date',\n",
    "#                       'OPEN',\n",
    "#                       'HIGH',\n",
    "#                       'LOW',\n",
    "#                       'CLOSE',\n",
    "#                       'VOLUME',\n",
    "#                       'flair_sentiment_content_score']].astype(np.float64)\n",
    "\n",
    "new_df_flair_content['Year'] = pd.DatetimeIndex(new_df_flair_content['Date']).year\n",
    "new_df_flair_content['Month'] = pd.DatetimeIndex(new_df_flair_content['Date']).month\n",
    "new_df_flair_content['Day'] = pd.DatetimeIndex(new_df_flair_content['Date']).day\n",
    "new_df_flair_content['Hour'] = pd.DatetimeIndex(new_df_flair_content['Date']).hour\n",
    "new_df_flair_content['Minute'] = pd.DatetimeIndex(new_df_flair_content['Date']).minute\n",
    "new_df_flair_content['Second'] = pd.DatetimeIndex(new_df_flair_content['Date']).second\n",
    "\n",
    "new_df_flair_content = new_df_flair_content.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_flair_content = 0.1\n",
    "\n",
    "X_train_flair_content, \\\n",
    "X_else_flair_content,\\\n",
    "y_train_flair_content, \\\n",
    "y_else_flair_content = train_test_split(new_df_flair_content,\n",
    "                                        new_df_flair_content['OPEN'],\n",
    "                                        test_size=valid_test_size_split_flair_content*2,\n",
    "                                        shuffle=False)\n",
    "\n",
    "X_valid_flair_content, \\\n",
    "X_test_flair_content, \\\n",
    "y_valid_flair_content, \\\n",
    "y_test_flair_content = train_test_split(X_else_flair_content,\n",
    "                                        y_else_flair_content,\n",
    "                                        test_size=0.5,\n",
    "                                        shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_flair_content(df_x, series_y, normalizers_flair_content = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'flair_sentiment_content_score']\n",
    "\n",
    "    if not normalizers_flair_content:\n",
    "        normalizers_flair_content = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_flair_content:\n",
    "            normalizers_flair_content[feat] = MinMaxScaler()\n",
    "            normalizers_flair_content[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_flair_content[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_flair_content['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_flair_content\n",
    "\n",
    "X_train_norm_flair_content, \\\n",
    "y_train_norm_flair_content, \\\n",
    "normalizers_flair_content = minmax_scale_flair_content(X_train_flair_content,\n",
    "                                                       y_train_flair_content\n",
    "                                                       )\n",
    "\n",
    "X_valid_norm_flair_content, \\\n",
    "y_valid_norm_flair_content, \\\n",
    "_ = minmax_scale_flair_content(X_valid_flair_content,\n",
    "                               y_valid_flair_content,\n",
    "                               normalizers_flair_content=normalizers_flair_content\n",
    "                               )\n",
    "\n",
    "X_test_norm_flair_content, \\\n",
    "y_test_norm_flair_content, \\\n",
    "_ = minmax_scale_flair_content(X_test_flair_content,\n",
    "                               y_test_flair_content,\n",
    "                               normalizers_flair_content=normalizers_flair_content\n",
    "                               )\n",
    "\n",
    "def encode_cyclicals_flair_content(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_flair_content = encode_cyclicals_flair_content(X_train_norm_flair_content)\n",
    "X_valid_norm_flair_content = encode_cyclicals_flair_content(X_valid_norm_flair_content)\n",
    "X_test_norm_flair_content = encode_cyclicals_flair_content(X_test_norm_flair_content)\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_flair_content = 45\n",
    "FORECAST_DISTANCE_flair_content = 9\n",
    "\n",
    "segmenter_flair_content = SegmentXYForecast(width=TIME_WINDOW_flair_content,\n",
    "                                            step=1,\n",
    "                                            y_func=last,\n",
    "                                            forecast=FORECAST_DISTANCE_flair_content\n",
    "                                            )\n",
    "\n",
    "X_train_rolled_flair_content, \\\n",
    "y_train_rolled_flair_content, \\\n",
    "_ = segmenter_flair_content.fit_transform([X_train_norm_flair_content.values],\n",
    "                                          [y_train_norm_flair_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "X_valid_rolled_flair_content, \\\n",
    "y_valid_rolled_flair_content, \\\n",
    "_ = segmenter_flair_content.fit_transform([X_valid_norm_flair_content.values],\n",
    "                                          [y_valid_norm_flair_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "X_test_rolled_flair_content,\\\n",
    "y_test_rolled_flair_content, \\\n",
    "_ = segmenter_flair_content.fit_transform([X_test_norm_flair_content.values],\n",
    "                                          [y_test_norm_flair_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "# LSTM Model\n",
    "first_lstm_size_flair_content = 75\n",
    "second_lstm_size_flair_content = 40\n",
    "dropout_flair_content = 0.1\n",
    "EPOCHS_flair_content = 50\n",
    "BATCH_SIZE_flair_content = 32\n",
    "column_count_flair_content = len(X_train_norm_flair_content.columns)\n",
    "# model with use of Funcational API of Keras\n",
    "# input layer\n",
    "input_layer_flair_content = Input(shape=(TIME_WINDOW_flair_content, column_count_flair_content))\n",
    "# first LSTM layer\n",
    "first_lstm_flair_content = LSTM(first_lstm_size_flair_content,\n",
    "                                return_sequences=True,\n",
    "                                dropout=dropout_flair_content)(input_layer_flair_content)\n",
    "# second LTSM layer\n",
    "second_lstm_flair_content = LSTM(second_lstm_size_flair_content,\n",
    "                                 return_sequences=False,\n",
    "                                 dropout=dropout_flair_content)(first_lstm_flair_content)\n",
    "# output layer\n",
    "output_layer_flair_content = Dense(1)(second_lstm_flair_content)\n",
    "# creating Model\n",
    "model_flair_content = Model(inputs=input_layer_flair_content, outputs=output_layer_flair_content)\n",
    "# compile model\n",
    "model_flair_content.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "# model summary\n",
    "model_flair_content.summary()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# fitting model\n",
    "hist_flair_content = model_flair_content.fit(x=X_train_rolled_flair_content,\n",
    "                                             y=y_train_rolled_flair_content,\n",
    "                                             batch_size=BATCH_SIZE_flair_content,\n",
    "                                             validation_data=(X_valid_rolled_flair_content,\n",
    "                                                              y_valid_rolled_flair_content\n",
    "                                                              ),\n",
    "                                             epochs=EPOCHS_flair_content,\n",
    "                                             verbose=1,\n",
    "                                             shuffle=False\n",
    "                                             )\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "plt.plot(hist_flair_content.history['loss'], label='train_flair_content')\n",
    "plt.plot(hist_flair_content.history['val_loss'], label='test_flair_content')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "rms_LSTM_flair_content = math.sqrt(min(hist_flair_content.history['val_loss']))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# predicting stock prices\n",
    "predicted_stock_price_flair_content = model_flair_content.predict(X_test_rolled_flair_content)\n",
    "\n",
    "predicted_stock_price_flair_content = normalizers_flair_content['OPEN']\\\n",
    "                                      .inverse_transform(predicted_stock_price_flair_content).reshape(-1, 1)\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid:\", rms_LSTM_flair_content)\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\",\n",
    "      normalizers_flair_content[\"OPEN\"].inverse_transform(np.array([rms_LSTM_flair_content]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(predicted_stock_price_flair_content)\n",
    "\n",
    "### analysis with flair header\n",
    "new_df_flair_header = concatenate_dataframe[['Date',\n",
    "                                             'OPEN',\n",
    "                                             'HIGH',\n",
    "                                             'LOW',\n",
    "                                             'CLOSE',\n",
    "                                             'VOLUME',\n",
    "                                             'flair_sentiment_header_score']]\n",
    "\n",
    "new_df_flair_header['flair_sentiment_header_score'] = new_df_flair_header['flair_sentiment_header_score'].fillna(0)\n",
    "# new_df_flair_header[['Date',\n",
    "#                      'OPEN',\n",
    "#                      'HIGH',\n",
    "#                      'LOW',\n",
    "#                      'CLOSE',\n",
    "#                      'VOLUME',\n",
    "#                      'flair_sentiment_header_score']].astype(np.float64)\n",
    "\n",
    "new_df_flair_header['Year'] = pd.DatetimeIndex(new_df_flair_header['Date']).year\n",
    "new_df_flair_header['Month'] = pd.DatetimeIndex(new_df_flair_header['Date']).month\n",
    "new_df_flair_header['Day'] = pd.DatetimeIndex(new_df_flair_header['Date']).day\n",
    "new_df_flair_header['Hour'] = pd.DatetimeIndex(new_df_flair_header['Date']).hour\n",
    "new_df_flair_header['Minute'] = pd.DatetimeIndex(new_df_flair_header['Date']).minute\n",
    "new_df_flair_header['Second'] = pd.DatetimeIndex(new_df_flair_header['Date']).second\n",
    "\n",
    "new_df_flair_header = new_df_flair_header.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_flair_header = 0.1\n",
    "\n",
    "X_train_flair_header, \\\n",
    "X_else_flair_header,\\\n",
    "y_train_flair_header, \\\n",
    "y_else_flair_header = train_test_split(new_df_flair_header,\n",
    "                                       new_df_flair_header['OPEN'],\n",
    "                                       test_size=valid_test_size_split_flair_header*2,\n",
    "                                       shuffle=False)\n",
    "\n",
    "X_valid_flair_header, \\\n",
    "X_test_flair_header, \\\n",
    "y_valid_flair_header, \\\n",
    "y_test_flair_header = train_test_split(X_else_flair_header,\n",
    "                                       y_else_flair_header,\n",
    "                                       test_size=0.5,\n",
    "                                       shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_flair_header(df_x, series_y, normalizers_flair_header = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'flair_sentiment_header_score']\n",
    "\n",
    "    if not normalizers_flair_header:\n",
    "        normalizers_flair_header = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_flair_header:\n",
    "            normalizers_flair_header[feat] = MinMaxScaler()\n",
    "            normalizers_flair_header[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_flair_header[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_flair_header['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_flair_header\n",
    "\n",
    "X_train_norm_flair_header, \\\n",
    "y_train_norm_flair_header, \\\n",
    "normalizers_flair_header = minmax_scale_flair_header(X_train_flair_header,\n",
    "                                                     y_train_flair_header\n",
    "                                                     )\n",
    "\n",
    "X_valid_norm_flair_header, \\\n",
    "y_valid_norm_flair_header, \\\n",
    "_ = minmax_scale_flair_header(X_valid_flair_header,\n",
    "                              y_valid_flair_header,\n",
    "                              normalizers_flair_header=normalizers_flair_header\n",
    "                              )\n",
    "\n",
    "X_test_norm_flair_header, \\\n",
    "y_test_norm_flair_header, \\\n",
    "_ = minmax_scale_flair_header(X_test_flair_header,\n",
    "                              y_test_flair_header,\n",
    "                              normalizers_flair_header=normalizers_flair_header\n",
    "                              )\n",
    "\n",
    "def encode_cyclicals_flair_header(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_flair_header = encode_cyclicals_flair_header(X_train_norm_flair_header)\n",
    "X_valid_norm_flair_header = encode_cyclicals_flair_header(X_valid_norm_flair_header)\n",
    "X_test_norm_flair_header = encode_cyclicals_flair_header(X_test_norm_flair_header)\n",
    "\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_flair_header = 45\n",
    "FORECAST_DISTANCE_flair_header = 9\n",
    "\n",
    "segmenter_flair_header = SegmentXYForecast(width=TIME_WINDOW_flair_header,\n",
    "                                           step=1,\n",
    "                                           y_func=last,\n",
    "                                           forecast=FORECAST_DISTANCE_flair_header\n",
    "                                           )\n",
    "\n",
    "X_train_rolled_flair_header, \\\n",
    "y_train_rolled_flair_header, \\\n",
    "_ = segmenter_flair_header.fit_transform([X_train_norm_flair_header.values],\n",
    "                                         [y_train_norm_flair_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "X_valid_rolled_flair_header, \\\n",
    "y_valid_rolled_flair_header, \\\n",
    "_ = segmenter_flair_header.fit_transform([X_valid_norm_flair_header.values],\n",
    "                            [y_valid_norm_flair_header.flatten()]\n",
    "                            )\n",
    "\n",
    "X_test_rolled_flair_header,\\\n",
    "y_test_rolled_flair_header, \\\n",
    "_ = segmenter_flair_header.fit_transform([X_test_norm_flair_header.values],\n",
    "                                         [y_test_norm_flair_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "# LSTM Model\n",
    "first_lstm_size_flair_header = 75\n",
    "second_lstm_size_flair_header = 40\n",
    "dropout_flair_header = 0.1\n",
    "EPOCHS_flair_header = 50\n",
    "BATCH_SIZE_flair_header = 32\n",
    "column_count_flair_header = len(X_train_norm_flair_header.columns)\n",
    "# model with use of Funcational API of Keras\n",
    "# input layer\n",
    "input_layer_flair_header = Input(shape=(TIME_WINDOW_flair_header, column_count_flair_header))\n",
    "# first LSTM layer\n",
    "first_lstm_flair_header = LSTM(first_lstm_size_flair_header,\n",
    "                               return_sequences=True,\n",
    "                               dropout=dropout_flair_header)(input_layer_flair_header)\n",
    "# second LTSM layer\n",
    "second_lstm_flair_header = LSTM(second_lstm_size_flair_header,\n",
    "                                 return_sequences=False,\n",
    "                                 dropout=dropout_flair_header)(first_lstm_flair_header)\n",
    "# output layer\n",
    "output_layer_flair_header = Dense(1)(second_lstm_flair_header)\n",
    "# creating Model\n",
    "model_flair_header = Model(inputs=input_layer_flair_header, outputs=output_layer_flair_header)\n",
    "# compile model\n",
    "model_flair_header.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "# model summary\n",
    "model_flair_header.summary()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# fitting model\n",
    "hist_flair_header = model_flair_header.fit(x=X_train_rolled_flair_header,\n",
    "                                           y=y_train_rolled_flair_header,\n",
    "                                           batch_size=BATCH_SIZE_flair_header,\n",
    "                                           validation_data=(X_valid_rolled_flair_header,\n",
    "                                                            y_valid_rolled_flair_header\n",
    "                                                            ),\n",
    "                                           epochs=EPOCHS_flair_header,\n",
    "                                           verbose=1,\n",
    "                                           shuffle=False\n",
    "                                           )\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "plt.plot(hist_flair_header.history['loss'], label='train_flair_header')\n",
    "plt.plot(hist_flair_header.history['val_loss'], label='test_flair_header')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "rms_LSTM_flair_header = math.sqrt(min(hist_flair_header.history['val_loss']))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# predicting stock prices\n",
    "predicted_stock_price_flair_header = model_flair_header.predict(X_test_rolled_flair_header)\n",
    "\n",
    "predicted_stock_price_flair_header = normalizers_flair_header['OPEN']\\\n",
    "                                      .inverse_transform(predicted_stock_price_flair_header).reshape(-1, 1)\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid:\", rms_LSTM_flair_header)\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\",\n",
    "      normalizers_flair_header[\"OPEN\"].inverse_transform(np.array([rms_LSTM_flair_header]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(predicted_stock_price_flair_header)\n",
    "\n",
    "### analysis with textblob sentiment content\n",
    "new_df_textblob_content = concatenate_dataframe[['Date',\n",
    "                                                 'OPEN',\n",
    "                                                 'HIGH',\n",
    "                                                 'LOW',\n",
    "                                                 'CLOSE',\n",
    "                                                 'VOLUME',\n",
    "                                                 'polarity_textblob_sentiment_content']]\n",
    "\n",
    "new_df_textblob_content['polarity_textblob_sentiment_content'] = new_df_textblob_content['polarity_textblob_sentiment_content'].fillna(0)\n",
    "# new_df_textblob_content[['Date',\n",
    "#                          'OPEN',\n",
    "#                          'HIGH',\n",
    "#                          'LOW',\n",
    "#                          'CLOSE',\n",
    "#                          'VOLUME',\n",
    "#                          'polarity_textblob_sentiment_content']].astype(np.float64)\n",
    "\n",
    "new_df_textblob_content['Year'] = pd.DatetimeIndex(new_df_textblob_content['Date']).year\n",
    "new_df_textblob_content['Month'] = pd.DatetimeIndex(new_df_textblob_content['Date']).month\n",
    "new_df_textblob_content['Day'] = pd.DatetimeIndex(new_df_textblob_content['Date']).day\n",
    "new_df_textblob_content['Hour'] = pd.DatetimeIndex(new_df_textblob_content['Date']).hour\n",
    "new_df_textblob_content['Minute'] = pd.DatetimeIndex(new_df_textblob_content['Date']).minute\n",
    "new_df_textblob_content['Second'] = pd.DatetimeIndex(new_df_textblob_content['Date']).second\n",
    "\n",
    "new_df_textblob_content = new_df_textblob_content.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_textblob_content = 0.1\n",
    "\n",
    "X_train_textblob_content, \\\n",
    "X_else_textblob_content,\\\n",
    "y_train_textblob_content, \\\n",
    "y_else_textblob_content = train_test_split(new_df_textblob_content,\n",
    "                                           new_df_textblob_content['OPEN'],\n",
    "                                           test_size=valid_test_size_split_textblob_content*2,\n",
    "                                           shuffle=False)\n",
    "\n",
    "X_valid_textblob_content, \\\n",
    "X_test_textblob_content, \\\n",
    "y_valid_textblob_content, \\\n",
    "y_test_textblob_content = train_test_split(X_else_textblob_content,\n",
    "                                           y_else_textblob_content,\n",
    "                                           test_size=0.5,\n",
    "                                           shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_textblob_content(df_x, series_y, normalizers_textblob_content = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'polarity_textblob_sentiment_content']\n",
    "\n",
    "    if not normalizers_textblob_content:\n",
    "        normalizers_textblob_content = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_textblob_content:\n",
    "            normalizers_textblob_content[feat] = MinMaxScaler()\n",
    "            normalizers_textblob_content[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_textblob_content[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_textblob_content['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_textblob_content\n",
    "\n",
    "X_train_norm_textblob_content, \\\n",
    "y_train_norm_textblob_content, \\\n",
    "normalizers_textblob_content = minmax_scale_textblob_content(X_train_textblob_content,\n",
    "                                                             y_train_textblob_content\n",
    "                                                             )\n",
    "\n",
    "X_valid_norm_textblob_content, \\\n",
    "y_valid_norm_textblob_content, \\\n",
    "_ = minmax_scale_textblob_content(X_valid_textblob_content,\n",
    "                                  y_valid_textblob_content,\n",
    "                                  normalizers_textblob_content=normalizers_textblob_content\n",
    "                                  )\n",
    "\n",
    "X_test_norm_textblob_content, \\\n",
    "y_test_norm_textblob_content, \\\n",
    "_ = minmax_scale_textblob_content(X_test_textblob_content,\n",
    "                                  y_test_textblob_content,\n",
    "                                  normalizers_textblob_content=normalizers_textblob_content\n",
    "                                  )\n",
    "\n",
    "def encode_cyclicals_textblob_content(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_textblob_content = encode_cyclicals_textblob_content(X_train_norm_textblob_content)\n",
    "X_valid_norm_textblob_content = encode_cyclicals_textblob_content(X_valid_norm_textblob_content)\n",
    "X_test_norm_textblob_content = encode_cyclicals_textblob_content(X_test_norm_textblob_content)\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_textblob_content = 45\n",
    "FORECAST_DISTANCE_textblob_content = 9\n",
    "\n",
    "segmenter_textblob_content = SegmentXYForecast(width=TIME_WINDOW_textblob_content,\n",
    "                                               step=1,\n",
    "                                               y_func=last,\n",
    "                                               forecast=FORECAST_DISTANCE_textblob_content\n",
    "                                               )\n",
    "\n",
    "X_train_rolled_textblob_content, \\\n",
    "y_train_rolled_textblob_content, \\\n",
    "_ = segmenter_textblob_content.fit_transform([X_train_norm_textblob_content.values],\n",
    "                                             [y_train_norm_textblob_content.flatten()]\n",
    "                                             )\n",
    "\n",
    "X_valid_rolled_textblob_content, \\\n",
    "y_valid_rolled_textblob_content, \\\n",
    "_ = segmenter_textblob_content.fit_transform([X_valid_norm_textblob_content.values],\n",
    "                                             [y_valid_norm_textblob_content.flatten()]\n",
    "                                             )\n",
    "\n",
    "X_test_rolled_textblob_content,\\\n",
    "y_test_rolled_textblob_content, \\\n",
    "_ = segmenter_textblob_content.fit_transform([X_test_norm_textblob_content.values],\n",
    "                                             [y_test_norm_textblob_content.flatten()]\n",
    "                                             )\n",
    "\n",
    "# LSTM Model\n",
    "first_lstm_size_textblob_content = 75\n",
    "second_lstm_size_textblob_content = 40\n",
    "dropout_textblob_content = 0.1\n",
    "EPOCHS_textblob_content = 50\n",
    "BATCH_SIZE_textblob_content = 32\n",
    "column_count_textblob_content = len(X_train_norm_textblob_content.columns)\n",
    "# model with use of Funcational API of Keras\n",
    "# input layer\n",
    "input_layer_textblob_content = Input(shape=(TIME_WINDOW_textblob_content, column_count_textblob_content))\n",
    "# first LSTM layer\n",
    "first_lstm_textblob_content = LSTM(first_lstm_size_textblob_content,\n",
    "                                   return_sequences=True,\n",
    "                                   dropout=dropout_textblob_content)(input_layer_textblob_content)\n",
    "# second LTSM layer\n",
    "second_lstm_textblob_content = LSTM(second_lstm_size_textblob_content,\n",
    "                                    return_sequences=False,\n",
    "                                    dropout=dropout_textblob_content)(first_lstm_textblob_content)\n",
    "# output layer\n",
    "output_layer_textblob_content = Dense(1)(second_lstm_textblob_content)\n",
    "# creating Model\n",
    "model_textblob_content = Model(inputs=input_layer_textblob_content, outputs=output_layer_textblob_content)\n",
    "# compile model\n",
    "model_textblob_content.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "# model summary\n",
    "model_textblob_content.summary()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# fitting model\n",
    "hist_textblob_content = model_textblob_content.fit(x=X_train_rolled_textblob_content,\n",
    "                                                   y=y_train_rolled_textblob_content,\n",
    "                                                   batch_size=BATCH_SIZE_textblob_content,\n",
    "                                                   validation_data=(X_valid_rolled_textblob_content,\n",
    "                                                                    y_valid_rolled_textblob_content\n",
    "                                                                    ),\n",
    "                                                   epochs=EPOCHS_textblob_content,\n",
    "                                                   verbose=1,\n",
    "                                                   shuffle=False\n",
    "                                                   )\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "plt.plot(hist_textblob_content.history['loss'], label='train_textblob_content')\n",
    "plt.plot(hist_textblob_content.history['val_loss'], label='test_textblob_content')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "rms_LSTM_textblob_content = math.sqrt(min(hist_textblob_content.history['val_loss']))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# predicting stock prices\n",
    "predicted_stock_price_textblob_content = model_textblob_content.predict(X_test_rolled_textblob_content)\n",
    "\n",
    "predicted_stock_price_textblob_content = normalizers_textblob_content['OPEN']\\\n",
    "                                      .inverse_transform(predicted_stock_price_textblob_content).reshape(-1, 1)\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid:\", rms_LSTM_textblob_content)\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\",\n",
    "      normalizers_textblob_content[\"OPEN\"].inverse_transform(np.array([rms_LSTM_textblob_content]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(predicted_stock_price_textblob_content)\n",
    "\n",
    "### analysis with textblob header\n",
    "new_df_textblob_header = concatenate_dataframe[['Date',\n",
    "                                                'OPEN',\n",
    "                                                'HIGH',\n",
    "                                                'LOW',\n",
    "                                                'CLOSE',\n",
    "                                                'VOLUME',\n",
    "                                                'polarity_textblob_sentiment_header']]\n",
    "\n",
    "new_df_textblob_header = new_df_textblob_header.fillna(0)\n",
    "# new_df_textblob_header[['Date',\n",
    "#                         'OPEN',\n",
    "#                         'HIGH',\n",
    "#                         'LOW',\n",
    "#                         'CLOSE',\n",
    "#                         'VOLUME',\n",
    "#                         'polarity_textblob_sentiment_header']].astype(np.float64)\n",
    "\n",
    "new_df_textblob_header['Year'] = pd.DatetimeIndex(new_df_textblob_header['Date']).year\n",
    "new_df_textblob_header['Month'] = pd.DatetimeIndex(new_df_textblob_header['Date']).month\n",
    "new_df_textblob_header['Day'] = pd.DatetimeIndex(new_df_textblob_header['Date']).day\n",
    "new_df_textblob_header['Hour'] = pd.DatetimeIndex(new_df_textblob_header['Date']).hour\n",
    "new_df_textblob_header['Minute'] = pd.DatetimeIndex(new_df_textblob_header['Date']).minute\n",
    "new_df_textblob_header['Second'] = pd.DatetimeIndex(new_df_textblob_header['Date']).second\n",
    "\n",
    "new_df_textblob_header = new_df_textblob_header.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_textblob_header = 0.1\n",
    "\n",
    "X_train_textblob_header, \\\n",
    "X_else_textblob_header,\\\n",
    "y_train_textblob_header, \\\n",
    "y_else_textblob_header = train_test_split(new_df_textblob_header,\n",
    "                                          new_df_textblob_header['OPEN'],\n",
    "                                          test_size=valid_test_size_split_textblob_header*2,\n",
    "                                          shuffle=False)\n",
    "\n",
    "X_valid_textblob_header, \\\n",
    "X_test_textblob_header, \\\n",
    "y_valid_textblob_header, \\\n",
    "y_test_textblob_header = train_test_split(X_else_textblob_header,\n",
    "                                          y_else_textblob_header,\n",
    "                                          test_size=0.5,\n",
    "                                          shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_textblob_header(df_x, series_y, normalizers_textblob_header = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'polarity_textblob_sentiment_header']\n",
    "\n",
    "    if not normalizers_textblob_header:\n",
    "        normalizers_textblob_header = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_textblob_header:\n",
    "            normalizers_textblob_header[feat] = MinMaxScaler()\n",
    "            normalizers_textblob_header[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_textblob_header[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_textblob_header['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_textblob_header\n",
    "\n",
    "X_train_norm_textblob_header, \\\n",
    "y_train_norm_textblob_header, \\\n",
    "normalizers_textblob_header = minmax_scale_textblob_header(X_train_textblob_header,\n",
    "                                                           y_train_textblob_header\n",
    "                                                           )\n",
    "\n",
    "X_valid_norm_textblob_header, \\\n",
    "y_valid_norm_textblob_header, \\\n",
    "_ = minmax_scale_textblob_header(X_valid_textblob_header,\n",
    "                                 y_valid_textblob_header,\n",
    "                                 normalizers_textblob_header=normalizers_textblob_header\n",
    "                                 )\n",
    "\n",
    "X_test_norm_textblob_header, \\\n",
    "y_test_norm_textblob_header, \\\n",
    "_ = minmax_scale_textblob_header(X_test_textblob_header,\n",
    "                                 y_test_textblob_header,\n",
    "                                 normalizers_textblob_header=normalizers_textblob_header\n",
    "                                 )\n",
    "\n",
    "def encode_cyclicals_textblob_header(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_textblob_header = encode_cyclicals_textblob_header(X_train_norm_textblob_header)\n",
    "X_valid_norm_textblob_header = encode_cyclicals_textblob_header(X_valid_norm_textblob_header)\n",
    "X_test_norm_textblob_header = encode_cyclicals_textblob_header(X_test_norm_textblob_header)\n",
    "\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_textblob_header = 45\n",
    "FORECAST_DISTANCE_textblob_header = 9\n",
    "\n",
    "segmenter_textblob_header = SegmentXYForecast(width=TIME_WINDOW_textblob_header,\n",
    "                                              step=1,\n",
    "                                              y_func=last,\n",
    "                                              forecast=FORECAST_DISTANCE_textblob_header\n",
    "                                              )\n",
    "\n",
    "X_train_rolled_textblob_header, \\\n",
    "y_train_rolled_textblob_header, \\\n",
    "_ = segmenter_textblob_header.fit_transform([X_train_norm_textblob_header.values],\n",
    "                                            [y_train_norm_textblob_header.flatten()]\n",
    "                                            )\n",
    "\n",
    "X_valid_rolled_textblob_header, \\\n",
    "y_valid_rolled_textblob_header, \\\n",
    "_ = segmenter_textblob_header.fit_transform([X_valid_norm_textblob_header.values],\n",
    "                                            [y_valid_norm_textblob_header.flatten()]\n",
    "                                            )\n",
    "\n",
    "X_test_rolled_textblob_header,\\\n",
    "y_test_rolled_textblob_header, \\\n",
    "_ = segmenter_textblob_header.fit_transform([X_test_norm_textblob_header.values],\n",
    "                                            [y_test_norm_textblob_header.flatten()]\n",
    "                                            )\n",
    "\n",
    "# LSTM Model\n",
    "first_lstm_size_textblob_header = 75\n",
    "second_lstm_size_textblob_header = 40\n",
    "dropout_textblob_header = 0.1\n",
    "EPOCHS_textblob_header = 50\n",
    "BATCH_SIZE_textblob_header = 32\n",
    "column_count_textblob_header = len(X_train_norm_textblob_header.columns)\n",
    "# model with use of Funcational API of Keras\n",
    "# input layer\n",
    "input_layer_textblob_header = Input(shape=(TIME_WINDOW_textblob_header, column_count_textblob_header))\n",
    "# first LSTM layer\n",
    "first_lstm_textblob_header = LSTM(first_lstm_size_textblob_header,\n",
    "                                  return_sequences=True,\n",
    "                                  dropout=dropout_textblob_header)(input_layer_textblob_header)\n",
    "# second LTSM layer\n",
    "second_lstm_textblob_header = LSTM(second_lstm_size_textblob_header,\n",
    "                                   return_sequences=False,\n",
    "                                   dropout=dropout_textblob_header)(first_lstm_textblob_header)\n",
    "# output layer\n",
    "output_layer_textblob_header = Dense(1)(second_lstm_textblob_header)\n",
    "# creating Model\n",
    "model_textblob_header = Model(inputs=input_layer_textblob_header, outputs=output_layer_textblob_header)\n",
    "# compile model\n",
    "model_textblob_header.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "# model summary\n",
    "model_textblob_header.summary()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# fitting model\n",
    "hist_textblob_header = model_textblob_header.fit(x=X_train_rolled_textblob_header,\n",
    "                                                 y=y_train_rolled_textblob_header,\n",
    "                                                 batch_size=BATCH_SIZE_textblob_header,\n",
    "                                                 validation_data=(X_valid_rolled_textblob_header,\n",
    "                                                                  y_valid_rolled_textblob_header\n",
    "                                                                  ),\n",
    "                                                 epochs=EPOCHS_textblob_header,\n",
    "                                                 verbose=1,\n",
    "                                                 shuffle=False\n",
    "                                                 )\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "plt.plot(hist_textblob_header.history['loss'], label='train_textblob_header')\n",
    "plt.plot(hist_textblob_header.history['val_loss'], label='test_textblob_header')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "rms_LSTM_textblob_header = math.sqrt(min(hist_textblob_header.history['val_loss']))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# predicting stock prices\n",
    "predicted_stock_price_textblob_header = model_textblob_header.predict(X_test_rolled_textblob_header)\n",
    "\n",
    "predicted_stock_price_textblob_header = normalizers_textblob_header['OPEN']\\\n",
    "                                      .inverse_transform(predicted_stock_price_textblob_header).reshape(-1, 1)\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid:\", rms_LSTM_textblob_header)\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\",\n",
    "      normalizers_textblob_header[\"OPEN\"].inverse_transform(np.array([rms_LSTM_textblob_header]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(predicted_stock_price_textblob_header)\n",
    "\n",
    "### analysis with vader sentiment content\n",
    "new_df_vader_content = concatenate_dataframe[['Date',\n",
    "                                              'OPEN',\n",
    "                                              'HIGH',\n",
    "                                              'LOW',\n",
    "                                              'CLOSE',\n",
    "                                              'VOLUME',\n",
    "                                              'compound_vader_articel_content']]\n",
    "\n",
    "new_df_vader_content['compound_vader_articel_content'] = new_df_vader_content['compound_vader_articel_content'].fillna(0)\n",
    "# new_df_vader_content[['Date',\n",
    "#                       'OPEN',\n",
    "#                       'HIGH',\n",
    "#                       'LOW',\n",
    "#                       'CLOSE',\n",
    "#                       'VOLUME',\n",
    "#                       'compound_vader_articel_content']].astype(np.float64)\n",
    "\n",
    "new_df_vader_content['Year'] = pd.DatetimeIndex(new_df_vader_content['Date']).year\n",
    "new_df_vader_content['Month'] = pd.DatetimeIndex(new_df_vader_content['Date']).month\n",
    "new_df_vader_content['Day'] = pd.DatetimeIndex(new_df_vader_content['Date']).day\n",
    "new_df_vader_content['Hour'] = pd.DatetimeIndex(new_df_vader_content['Date']).hour\n",
    "new_df_vader_content['Minute'] = pd.DatetimeIndex(new_df_vader_content['Date']).minute\n",
    "new_df_vader_content['Second'] = pd.DatetimeIndex(new_df_vader_content['Date']).second\n",
    "\n",
    "new_df_vader_content = new_df_vader_content.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_vader_content = 0.1\n",
    "\n",
    "X_train_vader_content, \\\n",
    "X_else_vader_content,\\\n",
    "y_train_vader_content, \\\n",
    "y_else_vader_content = train_test_split(new_df_vader_content,\n",
    "                                        new_df_vader_content['OPEN'],\n",
    "                                        test_size=valid_test_size_split_vader_content*2,\n",
    "                                        shuffle=False)\n",
    "\n",
    "X_valid_vader_content, \\\n",
    "X_test_vader_content, \\\n",
    "y_valid_vader_content, \\\n",
    "y_test_vader_content = train_test_split(X_else_vader_content,\n",
    "                                        y_else_vader_content,\n",
    "                                        test_size=0.5,\n",
    "                                        shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_vader_content(df_x, series_y, normalizers_vader_content = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'compound_vader_articel_content']\n",
    "\n",
    "    if not normalizers_vader_content:\n",
    "        normalizers_vader_content = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_vader_content:\n",
    "            normalizers_vader_content[feat] = MinMaxScaler()\n",
    "            normalizers_vader_content[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_vader_content[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_vader_content['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_vader_content\n",
    "\n",
    "X_train_norm_vader_content, \\\n",
    "y_train_norm_vader_content, \\\n",
    "normalizers_vader_content = minmax_scale_vader_content(X_train_vader_content,\n",
    "                                                       y_train_vader_content\n",
    "                                                       )\n",
    "\n",
    "X_valid_norm_vader_content, \\\n",
    "y_valid_norm_vader_content, \\\n",
    "_ = minmax_scale_vader_content(X_valid_vader_content,\n",
    "                               y_valid_vader_content,\n",
    "                               normalizers_vader_content=normalizers_vader_content\n",
    "                               )\n",
    "\n",
    "X_test_norm_vader_content, \\\n",
    "y_test_norm_vader_content, \\\n",
    "_ = minmax_scale_vader_content(X_test_vader_content,\n",
    "                               y_test_vader_content,\n",
    "                               normalizers_vader_content=normalizers_vader_content\n",
    "                               )\n",
    "\n",
    "def encode_cyclicals_vader_content(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_vader_content = encode_cyclicals_vader_content(X_train_norm_vader_content)\n",
    "X_valid_norm_vader_content = encode_cyclicals_vader_content(X_valid_norm_vader_content)\n",
    "X_test_norm_vader_content = encode_cyclicals_vader_content(X_test_norm_vader_content)\n",
    "\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_vader_content = 45\n",
    "FORECAST_DISTANCE_vader_content = 9\n",
    "\n",
    "segmenter_vader_content = SegmentXYForecast(width=TIME_WINDOW_vader_content,\n",
    "                                            step=1,\n",
    "                                            y_func=last,\n",
    "                                            forecast=FORECAST_DISTANCE_vader_content\n",
    "                                            )\n",
    "\n",
    "X_train_rolled_vader_content, \\\n",
    "y_train_rolled_vader_content, \\\n",
    "_ = segmenter_vader_content.fit_transform([X_train_norm_vader_content.values],\n",
    "                                          [y_train_norm_vader_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "X_valid_rolled_vader_content, \\\n",
    "y_valid_rolled_vader_content, \\\n",
    "_ = segmenter_vader_content.fit_transform([X_valid_norm_vader_content.values],\n",
    "                                          [y_valid_norm_vader_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "X_test_rolled_vader_content,\\\n",
    "y_test_rolled_vader_content, \\\n",
    "_ = segmenter_vader_content.fit_transform([X_test_norm_vader_content.values],\n",
    "                                          [y_test_norm_vader_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "# LSTM Model\n",
    "first_lstm_size_vader_content = 75\n",
    "second_lstm_size_vader_content = 40\n",
    "dropout_vader_content = 0.1\n",
    "EPOCHS_vader_content = 50\n",
    "BATCH_SIZE_vader_content = 32\n",
    "column_count_vader_content = len(X_train_norm_vader_content.columns)\n",
    "# model with use of Funcational API of Keras\n",
    "# input layer\n",
    "input_layer_vader_content = Input(shape=(TIME_WINDOW_vader_content, column_count_vader_content))\n",
    "# first LSTM layer\n",
    "first_lstm_vader_content = LSTM(first_lstm_size_vader_content,\n",
    "                                return_sequences=True,\n",
    "                                dropout=dropout_vader_content)(input_layer_vader_content)\n",
    "# second LTSM layer\n",
    "second_lstm_vader_content = LSTM(second_lstm_size_vader_content,\n",
    "                                 return_sequences=False,\n",
    "                                 dropout=dropout_vader_content)(first_lstm_vader_content)\n",
    "# output layer\n",
    "output_layer_vader_content = Dense(1)(second_lstm_vader_content)\n",
    "# creating Model\n",
    "model_vader_content = Model(inputs=input_layer_vader_content, outputs=output_layer_vader_content)\n",
    "# compile model\n",
    "model_vader_content.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "# model summary\n",
    "model_vader_content.summary()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# fitting model\n",
    "hist_vader_content = model_vader_content.fit(x=X_train_rolled_vader_content,\n",
    "                                             y=y_train_rolled_vader_content,\n",
    "                                             batch_size=BATCH_SIZE_vader_content,\n",
    "                                             validation_data=(X_valid_rolled_vader_content,\n",
    "                                                              y_valid_rolled_vader_content\n",
    "                                                              ),\n",
    "                                             epochs=EPOCHS_vader_content,\n",
    "                                             verbose=1,\n",
    "                                             shuffle=False\n",
    "                                             )\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "plt.plot(hist_vader_content.history['loss'], label='train_vader_content')\n",
    "plt.plot(hist_vader_content.history['val_loss'], label='test_vader_content')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "rms_LSTM_vader_content = math.sqrt(min(hist_vader_content.history['val_loss']))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# predicting stock prices\n",
    "predicted_stock_price_vader_content = model_vader_content.predict(X_test_rolled_vader_content)\n",
    "\n",
    "predicted_stock_price_vader_content = normalizers_vader_content['OPEN']\\\n",
    "                                      .inverse_transform(predicted_stock_price_vader_content).reshape(-1, 1)\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid:\", rms_LSTM_vader_content)\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\",\n",
    "      normalizers_vader_content[\"OPEN\"].inverse_transform(np.array([rms_LSTM_vader_content]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(predicted_stock_price_vader_content)\n",
    "\n",
    "### analysis with vader header\n",
    "new_df_vader_header = concatenate_dataframe[['Date',\n",
    "                                             'OPEN',\n",
    "                                             'HIGH',\n",
    "                                             'LOW',\n",
    "                                             'CLOSE',\n",
    "                                             'VOLUME',\n",
    "                                             'compound_vader_header']]\n",
    "\n",
    "new_df_vader_header = new_df_vader_header.fillna(0)\n",
    "# new_df_vader_header[['Date',\n",
    "#                      'OPEN',\n",
    "#                      'HIGH',\n",
    "#                      'LOW',\n",
    "#                      'CLOSE',\n",
    "#                      'VOLUME',\n",
    "#                      'compound_vader_header']].astype(np.float64)\n",
    "\n",
    "new_df_vader_header['Year'] = pd.DatetimeIndex(new_df_vader_header['Date']).year\n",
    "new_df_vader_header['Month'] = pd.DatetimeIndex(new_df_vader_header['Date']).month\n",
    "new_df_vader_header['Day'] = pd.DatetimeIndex(new_df_vader_header['Date']).day\n",
    "new_df_vader_header['Hour'] = pd.DatetimeIndex(new_df_vader_header['Date']).hour\n",
    "new_df_vader_header['Minute'] = pd.DatetimeIndex(new_df_vader_header['Date']).minute\n",
    "new_df_vader_header['Second'] = pd.DatetimeIndex(new_df_vader_header['Date']).second\n",
    "\n",
    "new_df_vader_header = new_df_vader_header.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_vader_header = 0.1\n",
    "\n",
    "X_train_vader_header, \\\n",
    "X_else_vader_header,\\\n",
    "y_train_vader_header, \\\n",
    "y_else_vader_header = train_test_split(new_df_vader_header,\n",
    "                                       new_df_vader_header['OPEN'],\n",
    "                                       test_size=valid_test_size_split_vader_header*2,\n",
    "                                       shuffle=False)\n",
    "\n",
    "X_valid_vader_header, \\\n",
    "X_test_vader_header, \\\n",
    "y_valid_vader_header, \\\n",
    "y_test_vader_header = train_test_split(X_else_vader_header,\n",
    "                                       y_else_vader_header,\n",
    "                                       test_size=0.5,\n",
    "                                       shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_vader_header(df_x, series_y, normalizers_vader_header = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'compound_vader_header']\n",
    "\n",
    "    if not normalizers_vader_header:\n",
    "        normalizers_vader_header = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_vader_header:\n",
    "            normalizers_vader_header[feat] = MinMaxScaler()\n",
    "            normalizers_vader_header[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_vader_header[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_vader_header['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_vader_header\n",
    "\n",
    "X_train_norm_vader_header, \\\n",
    "y_train_norm_vader_header, \\\n",
    "normalizers_vader_header = minmax_scale_vader_header(X_train_vader_header,\n",
    "                                                     y_train_vader_header\n",
    "                                                     )\n",
    "\n",
    "X_valid_norm_vader_header, \\\n",
    "y_valid_norm_vader_header, \\\n",
    "_ = minmax_scale_vader_header(X_valid_vader_header,\n",
    "                              y_valid_vader_header,\n",
    "                              normalizers_vader_header=normalizers_vader_header\n",
    "                              )\n",
    "\n",
    "X_test_norm_vader_header, \\\n",
    "y_test_norm_vader_header, \\\n",
    "_ = minmax_scale_vader_header(X_test_vader_header,\n",
    "                              y_test_vader_header,\n",
    "                              normalizers_vader_header=normalizers_vader_header\n",
    "                              )\n",
    "\n",
    "def encode_cyclicals_vader_header(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_vader_header = encode_cyclicals_vader_header(X_train_norm_vader_header)\n",
    "X_valid_norm_vader_header = encode_cyclicals_vader_header(X_valid_norm_vader_header)\n",
    "X_test_norm_vader_header = encode_cyclicals_vader_header(X_test_norm_vader_header)\n",
    "\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_vader_header = 45\n",
    "FORECAST_DISTANCE_vader_header = 9\n",
    "\n",
    "segmenter_vader_header = SegmentXYForecast(width=TIME_WINDOW_vader_header,\n",
    "                                           step=1,\n",
    "                                           y_func=last,\n",
    "                                           forecast=FORECAST_DISTANCE_vader_header\n",
    "                                           )\n",
    "\n",
    "X_train_rolled_vader_header, \\\n",
    "y_train_rolled_vader_header, \\\n",
    "_ = segmenter_vader_header.fit_transform([X_train_norm_vader_header.values],\n",
    "                                         [y_train_norm_vader_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "X_valid_rolled_vader_header, \\\n",
    "y_valid_rolled_vader_header, \\\n",
    "_ = segmenter_vader_header.fit_transform([X_valid_norm_vader_header.values],\n",
    "                                         [y_valid_norm_vader_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "X_test_rolled_vader_header,\\\n",
    "y_test_rolled_vader_header, \\\n",
    "_ = segmenter_vader_header.fit_transform([X_test_norm_vader_header.values],\n",
    "                                         [y_test_norm_vader_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "# LSTM Model\n",
    "first_lstm_size_vader_header = 75\n",
    "second_lstm_size_vader_header = 40\n",
    "dropout_vader_header = 0.1\n",
    "EPOCHS_vader_header = 50\n",
    "BATCH_SIZE_vader_header = 32\n",
    "column_count_vader_header = len(X_train_norm_vader_header.columns)\n",
    "# model with use of Funcational API of Keras\n",
    "# input layer\n",
    "input_layer_vader_header = Input(shape=(TIME_WINDOW_vader_header, column_count_vader_header))\n",
    "# first LSTM layer\n",
    "first_lstm_vader_header = LSTM(first_lstm_size_vader_header,\n",
    "                               return_sequences=True,\n",
    "                               dropout=dropout_vader_header)(input_layer_vader_header)\n",
    "# second LTSM layer\n",
    "second_lstm_vader_header = LSTM(second_lstm_size_vader_header,\n",
    "                                return_sequences=False,\n",
    "                                dropout=dropout_vader_header)(first_lstm_vader_header)\n",
    "# output layer\n",
    "output_layer_vader_header = Dense(1)(second_lstm_vader_header)\n",
    "# creating Model\n",
    "model_vader_header = Model(inputs=input_layer_vader_header, outputs=output_layer_vader_header)\n",
    "# compile model\n",
    "model_vader_header.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "# model summary\n",
    "model_vader_header.summary()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# fitting model\n",
    "hist_vader_header = model_vader_header.fit(x=X_train_rolled_vader_header,\n",
    "                                           y=y_train_rolled_vader_header,\n",
    "                                           batch_size=BATCH_SIZE_vader_header,\n",
    "                                           validation_data=(X_valid_rolled_vader_header,\n",
    "                                                            y_valid_rolled_vader_header\n",
    "                                                            ),\n",
    "                                           epochs=EPOCHS_vader_header,\n",
    "                                           verbose=1,\n",
    "                                           shuffle=False\n",
    "                                           )\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "plt.plot(hist_vader_header.history['loss'], label='train_vader_header')\n",
    "plt.plot(hist_vader_header.history['val_loss'], label='test_vader_header')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "rms_LSTM_vader_header = math.sqrt(min(hist_vader_header.history['val_loss']))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# predicting stock prices\n",
    "predicted_stock_price_vader_header = model_vader_header.predict(X_test_rolled_vader_header)\n",
    "\n",
    "predicted_stock_price_vader_header = normalizers_vader_header['OPEN']\\\n",
    "                                      .inverse_transform(predicted_stock_price_vader_header).reshape(-1, 1)\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid:\", rms_LSTM_vader_header)\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\",\n",
    "      normalizers_vader_header[\"OPEN\"].inverse_transform(np.array([rms_LSTM_vader_header]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(predicted_stock_price_vader_header)\n",
    "\n",
    "### analysis with without semantics\n",
    "new_df_without_semantics = concatenate_dataframe[['Date',\n",
    "                                                  'OPEN',\n",
    "                                                  'HIGH',\n",
    "                                                  'LOW',\n",
    "                                                  'CLOSE',\n",
    "                                                  'VOLUME']]\n",
    "\n",
    "new_df_without_semantics = new_df_without_semantics.fillna(0)\n",
    "# new_df_without_semantics[['Date',\n",
    "#                           'OPEN',\n",
    "#                           'HIGH',\n",
    "#                           'LOW',\n",
    "#                           'CLOSE',\n",
    "#                           'VOLUME']].astype(np.float64)\n",
    "\n",
    "new_df_without_semantics['Year'] = pd.DatetimeIndex(new_df_without_semantics['Date']).year\n",
    "new_df_without_semantics['Month'] = pd.DatetimeIndex(new_df_without_semantics['Date']).month\n",
    "new_df_without_semantics['Day'] = pd.DatetimeIndex(new_df_without_semantics['Date']).day\n",
    "new_df_without_semantics['Hour'] = pd.DatetimeIndex(new_df_without_semantics['Date']).hour\n",
    "new_df_without_semantics['Minute'] = pd.DatetimeIndex(new_df_without_semantics['Date']).minute\n",
    "new_df_without_semantics['Second'] = pd.DatetimeIndex(new_df_without_semantics['Date']).second\n",
    "\n",
    "new_df_without_semantics = new_df_without_semantics.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_without_semantics = 0.1\n",
    "\n",
    "X_train_without_semantics, \\\n",
    "X_else_without_semantics,\\\n",
    "y_train_without_semantics, \\\n",
    "y_else_without_semantics = train_test_split(new_df_without_semantics,\n",
    "                                            new_df_without_semantics['OPEN'],\n",
    "                                            test_size=valid_test_size_split_without_semantics*2,\n",
    "                                            shuffle=False)\n",
    "\n",
    "X_valid_without_semantics, \\\n",
    "X_test_without_semantics, \\\n",
    "y_valid_without_semantics, \\\n",
    "y_test_without_semantics = train_test_split(X_else_without_semantics,\n",
    "                                            y_else_without_semantics,\n",
    "                                            test_size=0.5,\n",
    "                                            shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_without_semantics(df_x, series_y, normalizers_without_semantics = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME']\n",
    "\n",
    "    if not normalizers_without_semantics:\n",
    "        normalizers_without_semantics = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_without_semantics:\n",
    "            normalizers_without_semantics[feat] = MinMaxScaler()\n",
    "            normalizers_without_semantics[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_without_semantics[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_without_semantics['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_without_semantics\n",
    "\n",
    "X_train_norm_without_semantics, \\\n",
    "y_train_norm_without_semantics, \\\n",
    "normalizers_without_semantics = minmax_scale_without_semantics(X_train_without_semantics,\n",
    "                                                               y_train_without_semantics\n",
    "                                                               )\n",
    "\n",
    "X_valid_norm_without_semantics, \\\n",
    "y_valid_norm_without_semantics, \\\n",
    "_ = minmax_scale_without_semantics(X_valid_without_semantics,\n",
    "                                   y_valid_without_semantics,\n",
    "                                   normalizers_without_semantics=normalizers_without_semantics\n",
    "                                   )\n",
    "\n",
    "X_test_norm_without_semantics, \\\n",
    "y_test_norm_without_semantics, \\\n",
    "_ = minmax_scale_without_semantics(X_test_without_semantics,\n",
    "                                   y_test_without_semantics,\n",
    "                                   normalizers_without_semantics=normalizers_without_semantics\n",
    "                                   )\n",
    "\n",
    "def encode_cyclicals_without_semantics(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_without_semantics = encode_cyclicals_without_semantics(X_train_norm_without_semantics)\n",
    "X_valid_norm_without_semantics = encode_cyclicals_without_semantics(X_valid_norm_without_semantics)\n",
    "X_test_norm_without_semantics = encode_cyclicals_without_semantics(X_test_norm_without_semantics)\n",
    "\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_without_semantics = 45\n",
    "FORECAST_DISTANCE_without_semantics = 9\n",
    "\n",
    "segmenter_without_semantics = SegmentXYForecast(width=TIME_WINDOW_without_semantics,\n",
    "                                                step=1,\n",
    "                                                y_func=last,\n",
    "                                                forecast=FORECAST_DISTANCE_without_semantics\n",
    "                                                )\n",
    "\n",
    "X_train_rolled_without_semantics, \\\n",
    "y_train_rolled_without_semantics, \\\n",
    "_ = segmenter_without_semantics.fit_transform([X_train_norm_without_semantics.values],\n",
    "                                              [y_train_norm_without_semantics.flatten()]\n",
    "                                              )\n",
    "\n",
    "X_valid_rolled_without_semantics, \\\n",
    "y_valid_rolled_without_semantics, \\\n",
    "_ = segmenter_without_semantics.fit_transform([X_valid_norm_without_semantics.values],\n",
    "                                              [y_valid_norm_without_semantics.flatten()]\n",
    "                                              )\n",
    "\n",
    "X_test_rolled_without_semantics,\\\n",
    "y_test_rolled_without_semantics, \\\n",
    "_ = segmenter_without_semantics.fit_transform([X_test_norm_without_semantics.values],\n",
    "                                              [y_test_norm_without_semantics.flatten()]\n",
    "                                              )\n",
    "\n",
    "# LSTM Model\n",
    "first_lstm_size_without_semantics = 75\n",
    "second_lstm_size_without_semantics = 40\n",
    "dropout_without_semantics = 0.1\n",
    "EPOCHS_without_semantics = 50\n",
    "BATCH_SIZE_without_semantics = 32\n",
    "column_count_without_semantics = len(X_train_norm_without_semantics.columns)\n",
    "# model with use of Funcational API of Keras\n",
    "# input layer\n",
    "input_layer_without_semantics = Input(shape=(TIME_WINDOW_without_semantics, column_count_without_semantics))\n",
    "# first LSTM layer\n",
    "first_lstm_without_semantics = LSTM(first_lstm_size_without_semantics,\n",
    "                                    return_sequences=True,\n",
    "                                    dropout=dropout_without_semantics)(input_layer_without_semantics)\n",
    "# second LTSM layer\n",
    "second_lstm_without_semantics = LSTM(second_lstm_size_without_semantics,\n",
    "                                     return_sequences=False,\n",
    "                                     dropout=dropout_without_semantics)(first_lstm_without_semantics)\n",
    "# output layer\n",
    "output_layer_without_semantics = Dense(1)(second_lstm_without_semantics)\n",
    "# creating Model\n",
    "model_without_semantics = Model(inputs=input_layer_without_semantics, outputs=output_layer_without_semantics)\n",
    "# compile model\n",
    "model_without_semantics.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "# model summary\n",
    "model_without_semantics.summary()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# fitting model\n",
    "hist_without_semantics = model_without_semantics.fit(x=X_train_rolled_without_semantics,\n",
    "                                                     y=y_train_rolled_without_semantics,\n",
    "                                                     batch_size=BATCH_SIZE_without_semantics,\n",
    "                                                     validation_data=(X_valid_rolled_without_semantics,\n",
    "                                                                      y_valid_rolled_without_semantics\n",
    "                                                                      ),\n",
    "                                                     epochs=EPOCHS_without_semantics,\n",
    "                                                     verbose=1,\n",
    "                                                     shuffle=False\n",
    "                                                     )\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "plt.plot(hist_without_semantics.history['loss'], label='train_without_semantics')\n",
    "plt.plot(hist_without_semantics.history['val_loss'], label='test_without_semantics')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "rms_LSTM_without_semantics = math.sqrt(min(hist_without_semantics.history['val_loss']))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "# predicting stock prices\n",
    "predicted_stock_price_without_semantics = model_without_semantics.predict(X_test_rolled_without_semantics)\n",
    "\n",
    "predicted_stock_price_without_semantics = normalizers_without_semantics['OPEN']\\\n",
    "                                      .inverse_transform(predicted_stock_price_without_semantics).reshape(-1, 1)\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid:\", rms_LSTM_without_semantics)\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\",\n",
    "      normalizers_without_semantics[\"OPEN\"].inverse_transform(np.array([rms_LSTM_without_semantics]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(predicted_stock_price_without_semantics)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "#plt.plot(X_test, color='black', label='renault Stock Price')\n",
    "plt.plot(predicted_stock_price_flair_content, color='green', label='Predicted Renault Stock Price with flair content analysis')\n",
    "plt.plot(predicted_stock_price_flair_header, color='red', label='Predicted Renault Stock Price with flair header analysis')\n",
    "plt.plot(predicted_stock_price_textblob_header, color='yellow', label='Predicted Renault Stock Price with textblob header analysis')\n",
    "plt.plot(predicted_stock_price_textblob_content, color='blue', label='Predicted Renault Stock Price with textblob content analysis')\n",
    "plt.plot(predicted_stock_price_vader_content, color='cyan', label='Predicted Renault Stock Price with vader content analysis')\n",
    "plt.plot(predicted_stock_price_vader_header, color='magenta', label='Predicted Renault Stock Price with vader header analysis')\n",
    "plt.plot(predicted_stock_price_without_semantics, color='orange', label='Predicted Renault Stock Price without semantics analysis')\n",
    "#plt.rcParams['figure.facecolor'] = 'salmon'\n",
    "plt.title('Renault Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Renault Stock Price')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.005), borderaxespad=8)\n",
    "date_today = str(datetime.now().strftime(\"%Y%m%d\"))\n",
    "plt.savefig(r'C:\\Users\\victo\\Master_Thesis\\stockprice_prediction\\LSTM\\renault\\hourly\\prediction_renault_' + date_today + '.png',\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=100,\n",
    "            pad_inches=1.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Run is finished and plot is saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
