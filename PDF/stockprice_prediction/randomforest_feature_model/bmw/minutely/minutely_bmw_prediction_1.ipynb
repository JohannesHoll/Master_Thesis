{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###necessary libaries###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seglearn.transform import FeatureRep, SegmentXYForecast, last\n",
    "from subprocess import check_output\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Flatten\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import newaxis\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import ParameterSampler, ParameterGrid\n",
    "\n",
    "model_seed = 100\n",
    "# ensure same output results\n",
    "seed(101)\n",
    "tf.random.set_seed(model_seed)\n",
    "\n",
    "# file where csv files lies\n",
    "path = r'C:\\Users\\victo\\Master_Thesis\\merging_data\\bmw\\minutely\\merged_files'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# read files to pandas frame\n",
    "list_of_files = []\n",
    "\n",
    "for filename in all_files:\n",
    "    list_of_files.append(pd.read_csv(filename,\n",
    "                                     sep=',',\n",
    "                                     )\n",
    "                         )\n",
    "\n",
    "# Concatenate all content of files into one DataFrames\n",
    "concatenate_dataframe = pd.concat(list_of_files,\n",
    "                                  ignore_index=True,\n",
    "                                  axis=0,\n",
    "                                  )\n",
    "\n",
    "### analysis with flair sentiment content\n",
    "new_df_flair_content = concatenate_dataframe[['Date',\n",
    "                                              'OPEN',\n",
    "                                              'HIGH',\n",
    "                                              'LOW',\n",
    "                                              'CLOSE',\n",
    "                                              'VOLUME',\n",
    "                                              'flair_sentiment_content_score']]\n",
    "\n",
    "new_df_flair_content = new_df_flair_content.fillna(0)\n",
    "# new_df_flair_content[['Date',\n",
    "#                       'OPEN',\n",
    "#                       'HIGH',\n",
    "#                       'LOW',\n",
    "#                       'CLOSE',\n",
    "#                       'VOLUME',\n",
    "#                       'flair_sentiment_content_score']].astype(np.float64)\n",
    "\n",
    "new_df_flair_content['Year'] = pd.DatetimeIndex(new_df_flair_content['Date']).year\n",
    "new_df_flair_content['Month'] = pd.DatetimeIndex(new_df_flair_content['Date']).month\n",
    "new_df_flair_content['Day'] = pd.DatetimeIndex(new_df_flair_content['Date']).day\n",
    "new_df_flair_content['Hour'] = pd.DatetimeIndex(new_df_flair_content['Date']).hour\n",
    "new_df_flair_content['Minute'] = pd.DatetimeIndex(new_df_flair_content['Date']).minute\n",
    "new_df_flair_content['Second'] = pd.DatetimeIndex(new_df_flair_content['Date']).second\n",
    "\n",
    "new_df_flair_content = new_df_flair_content.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_flair_content = 0.1\n",
    "\n",
    "X_train_flair_content, \\\n",
    "X_else_flair_content,\\\n",
    "y_train_flair_content, \\\n",
    "y_else_flair_content = train_test_split(new_df_flair_content,\n",
    "                                        new_df_flair_content['OPEN'],\n",
    "                                        test_size=valid_test_size_split_flair_content*2,\n",
    "                                        shuffle=False)\n",
    "\n",
    "X_valid_flair_content, \\\n",
    "X_test_flair_content, \\\n",
    "y_valid_flair_content, \\\n",
    "y_test_flair_content = train_test_split(X_else_flair_content,\n",
    "                                        y_else_flair_content,\n",
    "                                        test_size=0.5,\n",
    "                                        shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_flair_content(df_x, series_y, normalizers_flair_content = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'flair_sentiment_content_score']\n",
    "\n",
    "    if not normalizers_flair_content:\n",
    "        normalizers_flair_content = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_flair_content:\n",
    "            normalizers_flair_content[feat] = MinMaxScaler()\n",
    "            normalizers_flair_content[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_flair_content[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_flair_content['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_flair_content\n",
    "\n",
    "X_train_norm_flair_content, \\\n",
    "y_train_norm_flair_content, \\\n",
    "normalizers_flair_content = minmax_scale_flair_content(X_train_flair_content,\n",
    "                                                       y_train_flair_content\n",
    "                                                       )\n",
    "\n",
    "X_valid_norm_flair_content, \\\n",
    "y_valid_norm_flair_content, \\\n",
    "_ = minmax_scale_flair_content(X_valid_flair_content,\n",
    "                               y_valid_flair_content,\n",
    "                               normalizers_flair_content=normalizers_flair_content\n",
    "                               )\n",
    "\n",
    "X_test_norm_flair_content, \\\n",
    "y_test_norm_flair_content, \\\n",
    "_ = minmax_scale_flair_content(X_test_flair_content,\n",
    "                               y_test_flair_content,\n",
    "                               normalizers_flair_content=normalizers_flair_content\n",
    "                               )\n",
    "\n",
    "def encode_cyclicals_flair_content(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_flair_content = encode_cyclicals_flair_content(X_train_norm_flair_content)\n",
    "X_valid_norm_flair_content = encode_cyclicals_flair_content(X_valid_norm_flair_content)\n",
    "X_test_norm_flair_content = encode_cyclicals_flair_content(X_test_norm_flair_content)\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_flair_content = 60\n",
    "FORECAST_DISTANCE_flair_content = 30\n",
    "\n",
    "segmenter_flair_content = SegmentXYForecast(width=TIME_WINDOW_flair_content,\n",
    "                                            step=1,\n",
    "                                            y_func=last,\n",
    "                                            forecast=FORECAST_DISTANCE_flair_content\n",
    "                                            )\n",
    "\n",
    "X_train_rolled_flair_content, \\\n",
    "y_train_rolled_flair_content, \\\n",
    "_ = segmenter_flair_content.fit_transform([X_train_norm_flair_content.values],\n",
    "                                          [y_train_norm_flair_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "X_valid_rolled_flair_content, \\\n",
    "y_valid_rolled_flair_content, \\\n",
    "_ = segmenter_flair_content.fit_transform([X_valid_norm_flair_content.values],\n",
    "                                          [y_valid_norm_flair_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "X_test_rolled_flair_content, \\\n",
    "y_test_rolled_flair_content, \\\n",
    "_ = segmenter_flair_content.fit_transform([X_test_norm_flair_content.values],\n",
    "                                          [y_test_norm_flair_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "shape_flair_content = X_train_rolled_flair_content.shape\n",
    "X_train_flattened_flair_content = X_train_rolled_flair_content.reshape(shape_flair_content[0],\n",
    "                                                                       shape_flair_content[1]*shape_flair_content[2]\n",
    "                                                                       )\n",
    "X_train_flattened_flair_content.shape\n",
    "shape_flair_content = X_valid_rolled_flair_content.shape\n",
    "X_valid_flattened = X_valid_rolled_flair_content.reshape(shape_flair_content[0],\n",
    "                                                         shape_flair_content[1]*shape_flair_content[2]\n",
    "                                                         )\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "N_ESTIMATORS_flair_content = 30\n",
    "RANDOM_STATE_flair_content = 452543634\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "RF_feature_model_flair_content = RandomForestRegressor(random_state=RANDOM_STATE_flair_content,\n",
    "                                                       n_estimators=N_ESTIMATORS_flair_content,\n",
    "                                                       n_jobs=-1,\n",
    "                                                       verbose=100\n",
    "                                                       )\n",
    "\n",
    "feature_converter_flair_content = FeatureRep()\n",
    "RF_feature_model_flair_content.fit(feature_converter_flair_content.fit_transform(X_train_rolled_flair_content),\n",
    "                                   y_train_rolled_flair_content\n",
    "                                   )\n",
    "\n",
    "RF_feature_model_predictions_flair_content = RF_feature_model_flair_content.predict(feature_converter_flair_content.transform(X_valid_rolled_flair_content)\n",
    "                                                                                    )\n",
    "\n",
    "rms_feature_flair_content = sqrt(mean_squared_error(y_valid_rolled_flair_content, RF_feature_model_predictions_flair_content))\n",
    "\n",
    "print(\"Root mean squared error on valid:\",rms_feature_flair_content)\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\",normalizers_flair_content[\"OPEN\"]\n",
    "      .inverse_transform(np.array([rms_feature_flair_content]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "RF_feature_model_predictions_flair_content = normalizers_flair_content['OPEN']\\\n",
    "                                .inverse_transform(np.array(RF_feature_model_predictions_flair_content).reshape(-1, 1))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "### analysis with flair header\n",
    "new_df_flair_header = concatenate_dataframe[['Date',\n",
    "                                             'OPEN',\n",
    "                                             'HIGH',\n",
    "                                             'LOW',\n",
    "                                             'CLOSE',\n",
    "                                             'VOLUME',\n",
    "                                             'flair_sentiment_header_score']]\n",
    "\n",
    "new_df_flair_header = new_df_flair_header.fillna(0)\n",
    "# new_df_flair_header[['Date',\n",
    "#                      'OPEN',\n",
    "#                      'HIGH',\n",
    "#                      'LOW',\n",
    "#                      'CLOSE',\n",
    "#                      'VOLUME',\n",
    "#                      'flair_sentiment_header_score']].astype(np.float64)\n",
    "\n",
    "new_df_flair_header['Year'] = pd.DatetimeIndex(new_df_flair_header['Date']).year\n",
    "new_df_flair_header['Month'] = pd.DatetimeIndex(new_df_flair_header['Date']).month\n",
    "new_df_flair_header['Day'] = pd.DatetimeIndex(new_df_flair_header['Date']).day\n",
    "new_df_flair_header['Hour'] = pd.DatetimeIndex(new_df_flair_header['Date']).hour\n",
    "new_df_flair_header['Minute'] = pd.DatetimeIndex(new_df_flair_header['Date']).minute\n",
    "new_df_flair_header['Second'] = pd.DatetimeIndex(new_df_flair_header['Date']).second\n",
    "\n",
    "new_df_flair_header = new_df_flair_header.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_flair_header = 0.1\n",
    "\n",
    "X_train_flair_header, \\\n",
    "X_else_flair_header,\\\n",
    "y_train_flair_header, \\\n",
    "y_else_flair_header = train_test_split(new_df_flair_header,\n",
    "                                       new_df_flair_header['OPEN'],\n",
    "                                       test_size=valid_test_size_split_flair_header*2,\n",
    "                                       shuffle=False)\n",
    "\n",
    "X_valid_flair_header, \\\n",
    "X_test_flair_header, \\\n",
    "y_valid_flair_header, \\\n",
    "y_test_flair_header = train_test_split(X_else_flair_header,\n",
    "                                       y_else_flair_header,\n",
    "                                       test_size=0.5,\n",
    "                                       shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_flair_header(df_x, series_y, normalizers_flair_header = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'flair_sentiment_header_score']\n",
    "\n",
    "    if not normalizers_flair_header:\n",
    "        normalizers_flair_header = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_flair_header:\n",
    "            normalizers_flair_header[feat] = MinMaxScaler()\n",
    "            normalizers_flair_header[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_flair_header[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_flair_header['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_flair_header\n",
    "\n",
    "X_train_norm_flair_header, \\\n",
    "y_train_norm_flair_header, \\\n",
    "normalizers_flair_header = minmax_scale_flair_header(X_train_flair_header,\n",
    "                                                     y_train_flair_header\n",
    "                                                     )\n",
    "\n",
    "X_valid_norm_flair_header, \\\n",
    "y_valid_norm_flair_header, \\\n",
    "_ = minmax_scale_flair_header(X_valid_flair_header,\n",
    "                              y_valid_flair_header,\n",
    "                              normalizers_flair_header=normalizers_flair_header\n",
    "                              )\n",
    "\n",
    "X_test_norm_flair_header, \\\n",
    "y_test_norm_flair_header, \\\n",
    "_ = minmax_scale_flair_header(X_test_flair_header,\n",
    "                              y_test_flair_header,\n",
    "                              normalizers_flair_header=normalizers_flair_header\n",
    "                              )\n",
    "\n",
    "def encode_cyclicals_flair_header(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_flair_header = encode_cyclicals_flair_header(X_train_norm_flair_header)\n",
    "X_valid_norm_flair_header = encode_cyclicals_flair_header(X_valid_norm_flair_header)\n",
    "X_test_norm_flair_header = encode_cyclicals_flair_header(X_test_norm_flair_header)\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_flair_header = 60\n",
    "FORECAST_DISTANCE_flair_header = 30\n",
    "\n",
    "segmenter_flair_header = SegmentXYForecast(width=TIME_WINDOW_flair_header,\n",
    "                                           step=1,\n",
    "                                           y_func=last,\n",
    "                                           forecast=FORECAST_DISTANCE_flair_header\n",
    "                                           )\n",
    "\n",
    "X_train_rolled_flair_header, \\\n",
    "y_train_rolled_flair_header, \\\n",
    "_ = segmenter_flair_header.fit_transform([X_train_norm_flair_header.values],\n",
    "                                         [y_train_norm_flair_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "X_valid_rolled_flair_header, \\\n",
    "y_valid_rolled_flair_header, \\\n",
    "_ = segmenter_flair_header.fit_transform([X_valid_norm_flair_header.values],\n",
    "                                         [y_valid_norm_flair_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "X_test_rolled_flair_header, \\\n",
    "y_test_rolled_flair_header, \\\n",
    "_ = segmenter_flair_header.fit_transform([X_test_norm_flair_header.values],\n",
    "                                         [y_test_norm_flair_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "shape_flair_header = X_train_rolled_flair_header.shape\n",
    "X_train_flattened_flair_header = X_train_rolled_flair_header.reshape(shape_flair_header[0],\n",
    "                                                                     shape_flair_header[1]*shape_flair_header[2]\n",
    "                                                                     )\n",
    "\n",
    "X_train_flattened_flair_header.shape\n",
    "shape_flair_header = X_valid_rolled_flair_header.shape\n",
    "X_valid_flattened = X_valid_rolled_flair_header.reshape(shape_flair_header[0],\n",
    "                                                        shape_flair_header[1]*shape_flair_header[2]\n",
    "                                                        )\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "N_ESTIMATORS_flair_header = 30\n",
    "RANDOM_STATE_flair_header = 452543634\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "RF_feature_model_flair_header = RandomForestRegressor(random_state=RANDOM_STATE_flair_header,\n",
    "                                                      n_estimators=N_ESTIMATORS_flair_header,\n",
    "                                                      n_jobs=-1,\n",
    "                                                      verbose=100\n",
    "                                                      )\n",
    "\n",
    "feature_converter_flair_header = FeatureRep()\n",
    "RF_feature_model_flair_header.fit(feature_converter_flair_header.fit_transform(X_train_rolled_flair_header),\n",
    "                                  y_train_rolled_flair_header\n",
    "                                  )\n",
    "\n",
    "RF_feature_model_predictions_flair_header = RF_feature_model_flair_header.predict(feature_converter_flair_header.transform(X_valid_rolled_flair_header)\n",
    "                                                                                  )\n",
    "\n",
    "rms_feature_flair_header = sqrt(mean_squared_error(y_valid_rolled_flair_header,\n",
    "                                                   RF_feature_model_predictions_flair_header\n",
    "                                                   )\n",
    "                                )\n",
    "\n",
    "print(\"Root mean squared error on valid:\", rms_feature_flair_header)\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\", normalizers_flair_header[\"OPEN\"]\n",
    "      .inverse_transform(np.array([rms_feature_flair_header]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "RF_feature_model_predictions_flair_header = normalizers_flair_header['OPEN']\\\n",
    "                                .inverse_transform(np.array(RF_feature_model_predictions_flair_header).reshape(-1, 1))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "### analysis with textblob sentiment content\n",
    "new_df_textblob_content = concatenate_dataframe[['Date',\n",
    "                                                 'OPEN',\n",
    "                                                 'HIGH',\n",
    "                                                 'LOW',\n",
    "                                                 'CLOSE',\n",
    "                                                 'VOLUME',\n",
    "                                                 'polarity_textblob_sentiment_content']]\n",
    "\n",
    "new_df_textblob_content = new_df_textblob_content.fillna(0)\n",
    "# new_df_textblob_content[['Date',\n",
    "#                          'OPEN',\n",
    "#                          'HIGH',\n",
    "#                          'LOW',\n",
    "#                          'CLOSE',\n",
    "#                          'VOLUME',\n",
    "#                          'polarity_textblob_sentiment_content']].astype(np.float64)\n",
    "\n",
    "new_df_textblob_content['Year'] = pd.DatetimeIndex(new_df_textblob_content['Date']).year\n",
    "new_df_textblob_content['Month'] = pd.DatetimeIndex(new_df_textblob_content['Date']).month\n",
    "new_df_textblob_content['Day'] = pd.DatetimeIndex(new_df_textblob_content['Date']).day\n",
    "new_df_textblob_content['Hour'] = pd.DatetimeIndex(new_df_textblob_content['Date']).hour\n",
    "new_df_textblob_content['Minute'] = pd.DatetimeIndex(new_df_textblob_content['Date']).minute\n",
    "new_df_textblob_content['Second'] = pd.DatetimeIndex(new_df_textblob_content['Date']).second\n",
    "\n",
    "new_df_textblob_content = new_df_textblob_content.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_textblob_content = 0.1\n",
    "\n",
    "X_train_textblob_content, \\\n",
    "X_else_textblob_content,\\\n",
    "y_train_textblob_content, \\\n",
    "y_else_textblob_content = train_test_split(new_df_textblob_content,\n",
    "                                           new_df_textblob_content['OPEN'],\n",
    "                                           test_size=valid_test_size_split_textblob_content*2,\n",
    "                                           shuffle=False)\n",
    "\n",
    "X_valid_textblob_content, \\\n",
    "X_test_textblob_content, \\\n",
    "y_valid_textblob_content, \\\n",
    "y_test_textblob_content = train_test_split(X_else_textblob_content,\n",
    "                                           y_else_textblob_content,\n",
    "                                           test_size=0.5,\n",
    "                                           shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_textblob_content(df_x, series_y, normalizers_textblob_content = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'polarity_textblob_sentiment_content']\n",
    "\n",
    "    if not normalizers_textblob_content:\n",
    "        normalizers_textblob_content = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_textblob_content:\n",
    "            normalizers_textblob_content[feat] = MinMaxScaler()\n",
    "            normalizers_textblob_content[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_textblob_content[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_textblob_content['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_textblob_content\n",
    "\n",
    "X_train_norm_textblob_content, \\\n",
    "y_train_norm_textblob_content, \\\n",
    "normalizers_textblob_content = minmax_scale_textblob_content(X_train_textblob_content,\n",
    "                                                             y_train_textblob_content\n",
    "                                                             )\n",
    "\n",
    "X_valid_norm_textblob_content, \\\n",
    "y_valid_norm_textblob_content, \\\n",
    "_ = minmax_scale_textblob_content(X_valid_textblob_content,\n",
    "                                  y_valid_textblob_content,\n",
    "                                  normalizers_textblob_content=normalizers_textblob_content\n",
    "                                  )\n",
    "\n",
    "X_test_norm_textblob_content, \\\n",
    "y_test_norm_textblob_content, \\\n",
    "_ = minmax_scale_textblob_content(X_test_textblob_content,\n",
    "                                  y_test_textblob_content,\n",
    "                                  normalizers_textblob_content=normalizers_textblob_content\n",
    "                                  )\n",
    "\n",
    "def encode_cyclicals_textblob_content(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_textblob_content = encode_cyclicals_textblob_content(X_train_norm_textblob_content)\n",
    "X_valid_norm_textblob_content = encode_cyclicals_textblob_content(X_valid_norm_textblob_content)\n",
    "X_test_norm_textblob_content = encode_cyclicals_textblob_content(X_test_norm_textblob_content)\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_textblob_content = 60\n",
    "FORECAST_DISTANCE_textblob_content = 30\n",
    "\n",
    "segmenter_textblob_content = SegmentXYForecast(width=TIME_WINDOW_textblob_content,\n",
    "                                               step=1,\n",
    "                                               y_func=last,\n",
    "                                               forecast=FORECAST_DISTANCE_textblob_content\n",
    "                                               )\n",
    "\n",
    "X_train_rolled_textblob_content, \\\n",
    "y_train_rolled_textblob_content, \\\n",
    "_ = segmenter_textblob_content.fit_transform([X_train_norm_textblob_content.values],\n",
    "                                             [y_train_norm_textblob_content.flatten()]\n",
    "                                             )\n",
    "\n",
    "X_valid_rolled_textblob_content, \\\n",
    "y_valid_rolled_textblob_content, \\\n",
    "_ = segmenter_textblob_content.fit_transform([X_valid_norm_textblob_content.values],\n",
    "                                             [y_valid_norm_textblob_content.flatten()]\n",
    "                                             )\n",
    "\n",
    "X_test_rolled_textblob_content, \\\n",
    "y_test_rolled_textblob_content, \\\n",
    "_ = segmenter_textblob_content.fit_transform([X_test_norm_textblob_content.values],\n",
    "                                             [y_test_norm_textblob_content.flatten()]\n",
    "                                             )\n",
    "\n",
    "shape_textblob_content = X_train_rolled_textblob_content.shape\n",
    "X_train_flattened_textblob_content = X_train_rolled_textblob_content.reshape(shape_textblob_content[0],\n",
    "                                                                             shape_textblob_content[1]*shape_textblob_content[2]\n",
    "                                                                             )\n",
    "\n",
    "X_train_flattened_textblob_content.shape\n",
    "shape_textblob_content = X_valid_rolled_textblob_content.shape\n",
    "X_valid_flattened = X_valid_rolled_textblob_content.reshape(shape_textblob_content[0],\n",
    "                                                            shape_textblob_content[1]*shape_textblob_content[2]\n",
    "                                                            )\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "N_ESTIMATORS_textblob_content = 30\n",
    "RANDOM_STATE_textblob_content = 452543634\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "RF_feature_model_textblob_content = RandomForestRegressor(random_state=RANDOM_STATE_textblob_content,\n",
    "                                                          n_estimators=N_ESTIMATORS_textblob_content,\n",
    "                                                          n_jobs=-1,\n",
    "                                                          verbose=100\n",
    "                                                          )\n",
    "\n",
    "feature_converter_textblob_content = FeatureRep()\n",
    "RF_feature_model_textblob_content.fit(feature_converter_textblob_content.fit_transform(X_train_rolled_textblob_content),\n",
    "                                      y_train_rolled_textblob_content\n",
    "                                      )\n",
    "\n",
    "RF_feature_model_predictions_textblob_content = RF_feature_model_textblob_content.predict(feature_converter_textblob_content.transform(X_valid_rolled_textblob_content)\n",
    "                                                                                          )\n",
    "\n",
    "rms_feature_textblob_content = sqrt(mean_squared_error(y_valid_rolled_textblob_content,\n",
    "                                                       RF_feature_model_predictions_textblob_content\n",
    "                                                       )\n",
    "                                    )\n",
    "\n",
    "print(\"Root mean squared error on valid:\", rms_feature_textblob_content)\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\", normalizers_textblob_content[\"OPEN\"]\n",
    "      .inverse_transform(np.array([rms_feature_textblob_content]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "RF_feature_model_predictions_textblob_content = normalizers_textblob_content['OPEN']\\\n",
    "                                .inverse_transform(np.array(RF_feature_model_predictions_textblob_content).reshape(-1, 1))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "### analysis with textblob header\n",
    "new_df_textblob_header = concatenate_dataframe[['Date',\n",
    "                                                'OPEN',\n",
    "                                                'HIGH',\n",
    "                                                'LOW',\n",
    "                                                'CLOSE',\n",
    "                                                'VOLUME',\n",
    "                                                'polarity_textblob_sentiment_header']]\n",
    "\n",
    "new_df_textblob_header = new_df_textblob_header.fillna(0)\n",
    "# new_df_textblob_header[['Date',\n",
    "#                         'OPEN',\n",
    "#                         'HIGH',\n",
    "#                         'LOW',\n",
    "#                         'CLOSE',\n",
    "#                         'VOLUME',\n",
    "#                         'polarity_textblob_sentiment_header']].astype(np.float64)\n",
    "\n",
    "new_df_textblob_header['Year'] = pd.DatetimeIndex(new_df_textblob_header['Date']).year\n",
    "new_df_textblob_header['Month'] = pd.DatetimeIndex(new_df_textblob_header['Date']).month\n",
    "new_df_textblob_header['Day'] = pd.DatetimeIndex(new_df_textblob_header['Date']).day\n",
    "new_df_textblob_header['Hour'] = pd.DatetimeIndex(new_df_textblob_header['Date']).hour\n",
    "new_df_textblob_header['Minute'] = pd.DatetimeIndex(new_df_textblob_header['Date']).minute\n",
    "new_df_textblob_header['Second'] = pd.DatetimeIndex(new_df_textblob_header['Date']).second\n",
    "\n",
    "new_df_textblob_header = new_df_textblob_header.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_textblob_header = 0.1\n",
    "\n",
    "X_train_textblob_header, \\\n",
    "X_else_textblob_header,\\\n",
    "y_train_textblob_header, \\\n",
    "y_else_textblob_header = train_test_split(new_df_textblob_header,\n",
    "                                          new_df_textblob_header['OPEN'],\n",
    "                                          test_size=valid_test_size_split_textblob_header*2,\n",
    "                                          shuffle=False)\n",
    "\n",
    "X_valid_textblob_header, \\\n",
    "X_test_textblob_header, \\\n",
    "y_valid_textblob_header, \\\n",
    "y_test_textblob_header = train_test_split(X_else_textblob_header,\n",
    "                                          y_else_textblob_header,\n",
    "                                          test_size=0.5,\n",
    "                                          shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_textblob_header(df_x, series_y, normalizers_textblob_header = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'polarity_textblob_sentiment_header']\n",
    "\n",
    "    if not normalizers_textblob_header:\n",
    "        normalizers_textblob_header = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_textblob_header:\n",
    "            normalizers_textblob_header[feat] = MinMaxScaler()\n",
    "            normalizers_textblob_header[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_textblob_header[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_textblob_header['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_textblob_header\n",
    "\n",
    "X_train_norm_textblob_header, \\\n",
    "y_train_norm_textblob_header, \\\n",
    "normalizers_textblob_header = minmax_scale_textblob_header(X_train_textblob_header,\n",
    "                                                           y_train_textblob_header\n",
    "                                                           )\n",
    "\n",
    "X_valid_norm_textblob_header, \\\n",
    "y_valid_norm_textblob_header, \\\n",
    "_ = minmax_scale_textblob_header(X_valid_textblob_header,\n",
    "                                 y_valid_textblob_header,\n",
    "                                 normalizers_textblob_header=normalizers_textblob_header\n",
    "                                 )\n",
    "\n",
    "X_test_norm_textblob_header, \\\n",
    "y_test_norm_textblob_header, \\\n",
    "_ = minmax_scale_textblob_header(X_test_textblob_header,\n",
    "                                 y_test_textblob_header,\n",
    "                                 normalizers_textblob_header=normalizers_textblob_header\n",
    "                                 )\n",
    "\n",
    "def encode_cyclicals_textblob_header(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_textblob_header = encode_cyclicals_textblob_header(X_train_norm_textblob_header)\n",
    "X_valid_norm_textblob_header = encode_cyclicals_textblob_header(X_valid_norm_textblob_header)\n",
    "X_test_norm_textblob_header = encode_cyclicals_textblob_header(X_test_norm_textblob_header)\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_textblob_header = 60\n",
    "FORECAST_DISTANCE_textblob_header = 30\n",
    "\n",
    "segmenter_textblob_header = SegmentXYForecast(width=TIME_WINDOW_textblob_header,\n",
    "                                              step=1,\n",
    "                                              y_func=last,\n",
    "                                              forecast=FORECAST_DISTANCE_textblob_header\n",
    "                                              )\n",
    "\n",
    "X_train_rolled_textblob_header, \\\n",
    "y_train_rolled_textblob_header, \\\n",
    "_ = segmenter_textblob_header.fit_transform([X_train_norm_textblob_header.values],\n",
    "                                            [y_train_norm_textblob_header.flatten()]\n",
    "                                            )\n",
    "\n",
    "X_valid_rolled_textblob_header, \\\n",
    "y_valid_rolled_textblob_header, \\\n",
    "_ = segmenter_textblob_header.fit_transform([X_valid_norm_textblob_header.values],\n",
    "                                            [y_valid_norm_textblob_header.flatten()]\n",
    "                                            )\n",
    "\n",
    "X_test_rolled_textblob_header, \\\n",
    "y_test_rolled_textblob_header, \\\n",
    "_ = segmenter_textblob_header.fit_transform([X_test_norm_textblob_header.values],\n",
    "                                            [y_test_norm_textblob_header.flatten()]\n",
    "                                            )\n",
    "\n",
    "shape_textblob_header = X_train_rolled_textblob_header.shape\n",
    "X_train_flattened_textblob_header = X_train_rolled_textblob_header.reshape(shape_textblob_header[0],\n",
    "                                                                           shape_textblob_header[1]*shape_textblob_header[2]\n",
    "                                                                           )\n",
    "\n",
    "X_train_flattened_textblob_header.shape\n",
    "shape_textblob_header = X_valid_rolled_textblob_header.shape\n",
    "X_valid_flattened = X_valid_rolled_textblob_header.reshape(shape_textblob_header[0],\n",
    "                                                           shape_textblob_header[1]*shape_textblob_header[2]\n",
    "                                                           )\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "N_ESTIMATORS_textblob_header = 30\n",
    "RANDOM_STATE_textblob_header = 452543634\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "RF_feature_model_textblob_header = RandomForestRegressor(random_state=RANDOM_STATE_textblob_header,\n",
    "                                                         n_estimators=N_ESTIMATORS_textblob_header,\n",
    "                                                         n_jobs=-1,\n",
    "                                                         verbose=100\n",
    "                                                         )\n",
    "\n",
    "feature_converter_textblob_header = FeatureRep()\n",
    "RF_feature_model_textblob_header.fit(feature_converter_textblob_header.fit_transform(X_train_rolled_textblob_header),\n",
    "                                     y_train_rolled_textblob_header\n",
    "                                     )\n",
    "\n",
    "RF_feature_model_predictions_textblob_header = RF_feature_model_textblob_header.predict(feature_converter_textblob_header.transform(X_valid_rolled_textblob_header)\n",
    "                                                                                        )\n",
    "\n",
    "rms_feature_textblob_header = sqrt(mean_squared_error(y_valid_rolled_textblob_header,\n",
    "                                                      RF_feature_model_predictions_textblob_header\n",
    "                                                      )\n",
    "                                   )\n",
    "\n",
    "print(\"Root mean squared error on valid:\", rms_feature_textblob_header)\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\", normalizers_textblob_header[\"OPEN\"]\n",
    "      .inverse_transform(np.array([rms_feature_textblob_header]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "RF_feature_model_predictions_textblob_header = normalizers_textblob_header['OPEN']\\\n",
    "                                .inverse_transform(np.array(RF_feature_model_predictions_textblob_header).reshape(-1, 1))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "### analysis with vader sentiment content\n",
    "new_df_vader_content = concatenate_dataframe[['Date',\n",
    "                                              'OPEN',\n",
    "                                              'HIGH',\n",
    "                                              'LOW',\n",
    "                                              'CLOSE',\n",
    "                                              'VOLUME',\n",
    "                                              'compound_vader_articel_content']]\n",
    "\n",
    "new_df_vader_content = new_df_vader_content.fillna(0)\n",
    "# new_df_vader_content[['Date',\n",
    "#                       'OPEN',\n",
    "#                       'HIGH',\n",
    "#                       'LOW',\n",
    "#                       'CLOSE',\n",
    "#                       'VOLUME',\n",
    "#                       'compound_vader_articel_content']].astype(np.float64)\n",
    "\n",
    "new_df_vader_content['Year'] = pd.DatetimeIndex(new_df_vader_content['Date']).year\n",
    "new_df_vader_content['Month'] = pd.DatetimeIndex(new_df_vader_content['Date']).month\n",
    "new_df_vader_content['Day'] = pd.DatetimeIndex(new_df_vader_content['Date']).day\n",
    "new_df_vader_content['Hour'] = pd.DatetimeIndex(new_df_vader_content['Date']).hour\n",
    "new_df_vader_content['Minute'] = pd.DatetimeIndex(new_df_vader_content['Date']).minute\n",
    "new_df_vader_content['Second'] = pd.DatetimeIndex(new_df_vader_content['Date']).second\n",
    "\n",
    "new_df_vader_content = new_df_vader_content.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_vader_content = 0.1\n",
    "\n",
    "X_train_vader_content, \\\n",
    "X_else_vader_content,\\\n",
    "y_train_vader_content, \\\n",
    "y_else_vader_content = train_test_split(new_df_vader_content,\n",
    "                                        new_df_vader_content['OPEN'],\n",
    "                                        test_size=valid_test_size_split_vader_content*2,\n",
    "                                        shuffle=False)\n",
    "\n",
    "X_valid_vader_content, \\\n",
    "X_test_vader_content, \\\n",
    "y_valid_vader_content, \\\n",
    "y_test_vader_content = train_test_split(X_else_vader_content,\n",
    "                                        y_else_vader_content,\n",
    "                                        test_size=0.5,\n",
    "                                        shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_vader_content(df_x, series_y, normalizers_vader_content = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'compound_vader_articel_content']\n",
    "\n",
    "    if not normalizers_vader_content:\n",
    "        normalizers_vader_content = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_vader_content:\n",
    "            normalizers_vader_content[feat] = MinMaxScaler()\n",
    "            normalizers_vader_content[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_vader_content[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_vader_content['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_vader_content\n",
    "\n",
    "X_train_norm_vader_content, \\\n",
    "y_train_norm_vader_content, \\\n",
    "normalizers_vader_content = minmax_scale_vader_content(X_train_vader_content,\n",
    "                                                       y_train_vader_content\n",
    "                                                       )\n",
    "\n",
    "X_valid_norm_vader_content, \\\n",
    "y_valid_norm_vader_content, \\\n",
    "_ = minmax_scale_vader_content(X_valid_vader_content,\n",
    "                               y_valid_vader_content,\n",
    "                               normalizers_vader_content=normalizers_vader_content\n",
    "                               )\n",
    "\n",
    "X_test_norm_vader_content, \\\n",
    "y_test_norm_vader_content, \\\n",
    "_ = minmax_scale_vader_content(X_test_vader_content,\n",
    "                               y_test_vader_content,\n",
    "                               normalizers_vader_content=normalizers_vader_content\n",
    "                               )\n",
    "\n",
    "def encode_cyclicals_vader_content(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_vader_content = encode_cyclicals_vader_content(X_train_norm_vader_content)\n",
    "X_valid_norm_vader_content = encode_cyclicals_vader_content(X_valid_norm_vader_content)\n",
    "X_test_norm_vader_content = encode_cyclicals_vader_content(X_test_norm_vader_content)\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_vader_content = 60\n",
    "FORECAST_DISTANCE_vader_content = 30\n",
    "\n",
    "segmenter_vader_content = SegmentXYForecast(width=TIME_WINDOW_vader_content,\n",
    "                                            step=1,\n",
    "                                            y_func=last,\n",
    "                                            forecast=FORECAST_DISTANCE_vader_content\n",
    "                                            )\n",
    "\n",
    "X_train_rolled_vader_content, \\\n",
    "y_train_rolled_vader_content, \\\n",
    "_ = segmenter_vader_content.fit_transform([X_train_norm_vader_content.values],\n",
    "                                          [y_train_norm_vader_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "X_valid_rolled_vader_content, \\\n",
    "y_valid_rolled_vader_content, \\\n",
    "_ = segmenter_vader_content.fit_transform([X_valid_norm_vader_content.values],\n",
    "                                          [y_valid_norm_vader_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "X_test_rolled_vader_content, \\\n",
    "y_test_rolled_vader_content, \\\n",
    "_ = segmenter_vader_content.fit_transform([X_test_norm_vader_content.values],\n",
    "                                          [y_test_norm_vader_content.flatten()]\n",
    "                                          )\n",
    "\n",
    "shape_vader_content = X_train_rolled_vader_content.shape\n",
    "X_train_flattened_vader_content = X_train_rolled_vader_content.reshape(shape_vader_content[0],\n",
    "                                                                       shape_vader_content[1]*shape_vader_content[2]\n",
    "                                                                       )\n",
    "\n",
    "X_train_flattened_vader_content.shape\n",
    "shape_vader_content = X_valid_rolled_vader_content.shape\n",
    "X_valid_flattened = X_valid_rolled_vader_content.reshape(shape_vader_content[0],\n",
    "                                                         shape_vader_content[1]*shape_vader_content[2]\n",
    "                                                         )\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "N_ESTIMATORS_vader_content = 30\n",
    "RANDOM_STATE_vader_content = 452543634\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "RF_feature_model_vader_content = RandomForestRegressor(random_state=RANDOM_STATE_vader_content,\n",
    "                                                       n_estimators=N_ESTIMATORS_vader_content,\n",
    "                                                       n_jobs=-1,\n",
    "                                                       verbose=100\n",
    "                                                       )\n",
    "\n",
    "feature_converter_vader_content = FeatureRep()\n",
    "RF_feature_model_vader_content.fit(feature_converter_vader_content.fit_transform(X_train_rolled_vader_content),\n",
    "                                   y_train_rolled_vader_content\n",
    "                                   )\n",
    "\n",
    "RF_feature_model_predictions_vader_content = RF_feature_model_vader_content.predict(feature_converter_vader_content.transform(X_valid_rolled_vader_content)\n",
    "                                                                                  )\n",
    "\n",
    "rms_feature_vader_content = sqrt(mean_squared_error(y_valid_rolled_vader_content,\n",
    "                                                    RF_feature_model_predictions_vader_content\n",
    "                                                    )\n",
    "                                 )\n",
    "\n",
    "print(\"Root mean squared error on valid:\", rms_feature_vader_content)\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\", normalizers_vader_content[\"OPEN\"]\n",
    "      .inverse_transform(np.array([rms_feature_vader_content]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "RF_feature_model_predictions_vader_content = normalizers_vader_content['OPEN']\\\n",
    "                                .inverse_transform(np.array(RF_feature_model_predictions_vader_content).reshape(-1, 1))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "### analysis with vader header\n",
    "new_df_vader_header = concatenate_dataframe[['Date',\n",
    "                                             'OPEN',\n",
    "                                             'HIGH',\n",
    "                                             'LOW',\n",
    "                                             'CLOSE',\n",
    "                                             'VOLUME',\n",
    "                                             'compound_vader_header']]\n",
    "\n",
    "new_df_vader_header = new_df_vader_header.fillna(0)\n",
    "# new_df_vader_header[['Date',\n",
    "#                      'OPEN',\n",
    "#                      'HIGH',\n",
    "#                      'LOW',\n",
    "#                      'CLOSE',\n",
    "#                      'VOLUME',\n",
    "#                      'compound_vader_header']].astype(np.float64)\n",
    "\n",
    "new_df_vader_header['Year'] = pd.DatetimeIndex(new_df_vader_header['Date']).year\n",
    "new_df_vader_header['Month'] = pd.DatetimeIndex(new_df_vader_header['Date']).month\n",
    "new_df_vader_header['Day'] = pd.DatetimeIndex(new_df_vader_header['Date']).day\n",
    "new_df_vader_header['Hour'] = pd.DatetimeIndex(new_df_vader_header['Date']).hour\n",
    "new_df_vader_header['Minute'] = pd.DatetimeIndex(new_df_vader_header['Date']).minute\n",
    "new_df_vader_header['Second'] = pd.DatetimeIndex(new_df_vader_header['Date']).second\n",
    "\n",
    "new_df_vader_header = new_df_vader_header.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_vader_header = 0.1\n",
    "\n",
    "X_train_vader_header, \\\n",
    "X_else_vader_header,\\\n",
    "y_train_vader_header, \\\n",
    "y_else_vader_header = train_test_split(new_df_vader_header,\n",
    "                                       new_df_vader_header['OPEN'],\n",
    "                                       test_size=valid_test_size_split_vader_header*2,\n",
    "                                       shuffle=False)\n",
    "\n",
    "X_valid_vader_header, \\\n",
    "X_test_vader_header, \\\n",
    "y_valid_vader_header, \\\n",
    "y_test_vader_header = train_test_split(X_else_vader_header,\n",
    "                                       y_else_vader_header,\n",
    "                                       test_size=0.5,\n",
    "                                       shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_vader_header(df_x, series_y, normalizers_vader_header = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME',\n",
    "                          'compound_vader_header']\n",
    "\n",
    "    if not normalizers_vader_header:\n",
    "        normalizers_vader_header = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_vader_header:\n",
    "            normalizers_vader_header[feat] = MinMaxScaler()\n",
    "            normalizers_vader_header[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_vader_header[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_vader_header['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_vader_header\n",
    "\n",
    "X_train_norm_vader_header, \\\n",
    "y_train_norm_vader_header, \\\n",
    "normalizers_vader_header = minmax_scale_vader_header(X_train_vader_header,\n",
    "                                                     y_train_vader_header\n",
    "                                                     )\n",
    "\n",
    "X_valid_norm_vader_header, \\\n",
    "y_valid_norm_vader_header, \\\n",
    "_ = minmax_scale_vader_header(X_valid_vader_header,\n",
    "                              y_valid_vader_header,\n",
    "                              normalizers_vader_header=normalizers_vader_header\n",
    "                              )\n",
    "\n",
    "X_test_norm_vader_header, \\\n",
    "y_test_norm_vader_header, \\\n",
    "_ = minmax_scale_vader_header(X_test_vader_header,\n",
    "                              y_test_vader_header,\n",
    "                              normalizers_vader_header=normalizers_vader_header\n",
    "                              )\n",
    "\n",
    "def encode_cyclicals_vader_header(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_vader_header = encode_cyclicals_vader_header(X_train_norm_vader_header)\n",
    "X_valid_norm_vader_header = encode_cyclicals_vader_header(X_valid_norm_vader_header)\n",
    "X_test_norm_vader_header = encode_cyclicals_vader_header(X_test_norm_vader_header)\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_vader_header = 60\n",
    "FORECAST_DISTANCE_vader_header = 30\n",
    "\n",
    "segmenter_vader_header = SegmentXYForecast(width=TIME_WINDOW_vader_header,\n",
    "                                           step=1,\n",
    "                                           y_func=last,\n",
    "                                           forecast=FORECAST_DISTANCE_vader_header\n",
    "                                           )\n",
    "\n",
    "X_train_rolled_vader_header, \\\n",
    "y_train_rolled_vader_header, \\\n",
    "_ = segmenter_vader_header.fit_transform([X_train_norm_vader_header.values],\n",
    "                                         [y_train_norm_vader_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "X_valid_rolled_vader_header, \\\n",
    "y_valid_rolled_vader_header, \\\n",
    "_ = segmenter_vader_header.fit_transform([X_valid_norm_vader_header.values],\n",
    "                                         [y_valid_norm_vader_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "X_test_rolled_vader_header, \\\n",
    "y_test_rolled_vader_header, \\\n",
    "_ = segmenter_vader_header.fit_transform([X_test_norm_vader_header.values],\n",
    "                                         [y_test_norm_vader_header.flatten()]\n",
    "                                         )\n",
    "\n",
    "shape_vader_header = X_train_rolled_vader_header.shape\n",
    "X_train_flattened_vader_header = X_train_rolled_vader_header.reshape(shape_vader_header[0],\n",
    "                                                                     shape_vader_header[1]*shape_vader_header[2]\n",
    "                                                                     )\n",
    "\n",
    "X_train_flattened_vader_header.shape\n",
    "shape_vader_header = X_valid_rolled_vader_header.shape\n",
    "X_valid_flattened = X_valid_rolled_vader_header.reshape(shape_vader_header[0],\n",
    "                                                        shape_vader_header[1]*shape_vader_header[2]\n",
    "                                                        )\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "N_ESTIMATORS_vader_header = 30\n",
    "RANDOM_STATE_vader_header = 452543634\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "RF_feature_model_vader_header = RandomForestRegressor(random_state=RANDOM_STATE_vader_header,\n",
    "                                                      n_estimators=N_ESTIMATORS_vader_header,\n",
    "                                                      n_jobs=-1,\n",
    "                                                      verbose=100\n",
    "                                                      )\n",
    "\n",
    "feature_converter_vader_header = FeatureRep()\n",
    "RF_feature_model_vader_header.fit(feature_converter_vader_header.fit_transform(X_train_rolled_vader_header),\n",
    "                                  y_train_rolled_vader_header\n",
    "                                  )\n",
    "\n",
    "RF_feature_model_predictions_vader_header = RF_feature_model_vader_header.predict(feature_converter_vader_header.transform(X_valid_rolled_vader_header)\n",
    "                                                                                  )\n",
    "\n",
    "rms_feature_vader_header = sqrt(mean_squared_error(y_valid_rolled_vader_header,\n",
    "                                                   RF_feature_model_predictions_vader_header\n",
    "                                                   )\n",
    "                                )\n",
    "\n",
    "print(\"Root mean squared error on valid:\", rms_feature_vader_header)\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\", normalizers_vader_header[\"OPEN\"]\n",
    "      .inverse_transform(np.array([rms_feature_vader_header]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "RF_feature_model_predictions_vader_header = normalizers_vader_header['OPEN']\\\n",
    "                                .inverse_transform(np.array(RF_feature_model_predictions_vader_header).reshape(-1, 1))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "### analysis with without semantics\n",
    "new_df_without_semantics = concatenate_dataframe[['Date',\n",
    "                                                  'OPEN',\n",
    "                                                  'HIGH',\n",
    "                                                  'LOW',\n",
    "                                                  'CLOSE',\n",
    "                                                  'VOLUME']]\n",
    "\n",
    "new_df_without_semantics = new_df_without_semantics.fillna(0)\n",
    "# new_df_without_semantics[['Date',\n",
    "#                           'OPEN',\n",
    "#                           'HIGH',\n",
    "#                           'LOW',\n",
    "#                           'CLOSE',\n",
    "#                           'VOLUME']].astype(np.float64)\n",
    "\n",
    "new_df_without_semantics['Year'] = pd.DatetimeIndex(new_df_without_semantics['Date']).year\n",
    "new_df_without_semantics['Month'] = pd.DatetimeIndex(new_df_without_semantics['Date']).month\n",
    "new_df_without_semantics['Day'] = pd.DatetimeIndex(new_df_without_semantics['Date']).day\n",
    "new_df_without_semantics['Hour'] = pd.DatetimeIndex(new_df_without_semantics['Date']).hour\n",
    "new_df_without_semantics['Minute'] = pd.DatetimeIndex(new_df_without_semantics['Date']).minute\n",
    "new_df_without_semantics['Second'] = pd.DatetimeIndex(new_df_without_semantics['Date']).second\n",
    "\n",
    "new_df_without_semantics = new_df_without_semantics.drop(['Date'], axis=1)\n",
    "\n",
    "# train, valid, test split\n",
    "valid_test_size_split_without_semantics = 0.1\n",
    "\n",
    "X_train_without_semantics, \\\n",
    "X_else_without_semantics,\\\n",
    "y_train_without_semantics, \\\n",
    "y_else_without_semantics = train_test_split(new_df_without_semantics,\n",
    "                                            new_df_without_semantics['OPEN'],\n",
    "                                            test_size=valid_test_size_split_without_semantics*2,\n",
    "                                            shuffle=False)\n",
    "\n",
    "X_valid_without_semantics, \\\n",
    "X_test_without_semantics, \\\n",
    "y_valid_without_semantics, \\\n",
    "y_test_without_semantics = train_test_split(X_else_without_semantics,\n",
    "                                            y_else_without_semantics,\n",
    "                                            test_size=0.5,\n",
    "                                            shuffle=False)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def minmax_scale_without_semantics(df_x, series_y, normalizers_without_semantics = None):\n",
    "    features_to_minmax = ['Year',\n",
    "                          'Month',\n",
    "                          'Day',\n",
    "                          'Hour',\n",
    "                          'Minute',\n",
    "                          'Second',\n",
    "                          'OPEN',\n",
    "                          'HIGH',\n",
    "                          'LOW',\n",
    "                          'CLOSE',\n",
    "                          'VOLUME']\n",
    "\n",
    "    if not normalizers_without_semantics:\n",
    "        normalizers_without_semantics = {}\n",
    "\n",
    "    for feat in features_to_minmax:\n",
    "        if feat not in normalizers_without_semantics:\n",
    "            normalizers_without_semantics[feat] = MinMaxScaler()\n",
    "            normalizers_without_semantics[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "        df_x[feat] = normalizers_without_semantics[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "    series_y = normalizers_without_semantics['OPEN'].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "    return df_x, series_y, normalizers_without_semantics\n",
    "\n",
    "X_train_norm_without_semantics, \\\n",
    "y_train_norm_without_semantics, \\\n",
    "normalizers_without_semantics = minmax_scale_without_semantics(X_train_without_semantics,\n",
    "                                                               y_train_without_semantics\n",
    "                                                               )\n",
    "\n",
    "X_valid_norm_without_semantics, \\\n",
    "y_valid_norm_without_semantics, \\\n",
    "_ = minmax_scale_without_semantics(X_valid_without_semantics,\n",
    "                                   y_valid_without_semantics,\n",
    "                                   normalizers_without_semantics=normalizers_without_semantics\n",
    "                                   )\n",
    "\n",
    "X_test_norm_without_semantics, \\\n",
    "y_test_norm_without_semantics, \\\n",
    "_ = minmax_scale_without_semantics(X_test_without_semantics,\n",
    "                                   y_test_without_semantics,\n",
    "                                   normalizers_without_semantics=normalizers_without_semantics\n",
    "                                   )\n",
    "\n",
    "def encode_cyclicals_without_semantics(df_x):\n",
    "    # \"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
    "\n",
    "    #DIRECTIONS = {\"N\": 1.0, \"NE\": 2.0, \"E\": 3.0, \"SE\": 4.0, \"S\": 5.0, \"SW\": 6.0, \"W\": 7.0, \"NW\": 8.0, \"cv\": np.nan}\n",
    "\n",
    "    df_x['month_sin'] = np.sin(2 * np.pi * df_x.Month / 12)\n",
    "    df_x['month_cos'] = np.cos(2 * np.pi * df_x.Month / 12)\n",
    "    df_x.drop('Month', axis=1, inplace=True)\n",
    "\n",
    "    df_x['day_sin'] = np.sin(2 * np.pi * df_x.Day / 31)\n",
    "    df_x['day_cos'] = np.cos(2 * np.pi * df_x.Day / 31)\n",
    "    df_x.drop('Day', axis=1, inplace=True)\n",
    "\n",
    "    df_x['hour_sin'] = np.sin(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x['hour_cos'] = np.cos(2 * np.pi * df_x.Hour / 24)\n",
    "    df_x.drop('Hour', axis=1, inplace=True)\n",
    "\n",
    "    df_x['min_sin'] = np.sin(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x['min_cos'] = np.cos(2 * np.pi * df_x.Minute / 60)\n",
    "    df_x.drop('Minute', axis=1, inplace=True)\n",
    "\n",
    "    df_x['sec_sin'] = np.sin(2 * np.pi * df_x.Second / 60)\n",
    "    df_x['sec_cos'] = np.cos(2 * np.pi * df_x.Second / 60)\n",
    "    df_x.drop('Second', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df_x\n",
    "\n",
    "X_train_norm_without_semantics = encode_cyclicals_without_semantics(X_train_norm_without_semantics)\n",
    "X_valid_norm_without_semantics = encode_cyclicals_without_semantics(X_valid_norm_without_semantics)\n",
    "X_test_norm_without_semantics = encode_cyclicals_without_semantics(X_test_norm_without_semantics)\n",
    "\n",
    "# Creating target (y) and \"windows\" (X) for modeling\n",
    "TIME_WINDOW_without_semantics = 60\n",
    "FORECAST_DISTANCE_without_semantics = 30\n",
    "\n",
    "segmenter_without_semantics = SegmentXYForecast(width=TIME_WINDOW_without_semantics,\n",
    "                                           step=1,\n",
    "                                           y_func=last,\n",
    "                                           forecast=FORECAST_DISTANCE_without_semantics\n",
    "                                           )\n",
    "\n",
    "X_train_rolled_without_semantics, \\\n",
    "y_train_rolled_without_semantics, \\\n",
    "_ = segmenter_without_semantics.fit_transform([X_train_norm_without_semantics.values],\n",
    "                                         [y_train_norm_without_semantics.flatten()]\n",
    "                                         )\n",
    "\n",
    "X_valid_rolled_without_semantics, \\\n",
    "y_valid_rolled_without_semantics, \\\n",
    "_ = segmenter_without_semantics.fit_transform([X_valid_norm_without_semantics.values],\n",
    "                                         [y_valid_norm_without_semantics.flatten()]\n",
    "                                         )\n",
    "\n",
    "X_test_rolled_without_semantics, \\\n",
    "y_test_rolled_without_semantics, \\\n",
    "_ = segmenter_without_semantics.fit_transform([X_test_norm_without_semantics.values],\n",
    "                                         [y_test_norm_without_semantics.flatten()]\n",
    "                                         )\n",
    "\n",
    "shape_without_semantics = X_train_rolled_without_semantics.shape\n",
    "X_train_flattened_without_semantics = X_train_rolled_without_semantics.reshape(shape_without_semantics[0],\n",
    "                                                                     shape_without_semantics[1]*shape_without_semantics[2]\n",
    "                                                                     )\n",
    "\n",
    "X_train_flattened_without_semantics.shape\n",
    "shape_without_semantics = X_valid_rolled_without_semantics.shape\n",
    "X_valid_flattened = X_valid_rolled_without_semantics.reshape(shape_without_semantics[0],\n",
    "                                                        shape_without_semantics[1]*shape_without_semantics[2]\n",
    "                                                        )\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "N_ESTIMATORS_without_semantics = 30\n",
    "RANDOM_STATE_without_semantics = 452543634\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "RF_feature_model_without_semantics = RandomForestRegressor(random_state=RANDOM_STATE_without_semantics,\n",
    "                                                      n_estimators=N_ESTIMATORS_without_semantics,\n",
    "                                                      n_jobs=-1,\n",
    "                                                      verbose=100\n",
    "                                                      )\n",
    "\n",
    "feature_converter_without_semantics = FeatureRep()\n",
    "RF_feature_model_without_semantics.fit(feature_converter_without_semantics.fit_transform(X_train_rolled_without_semantics),\n",
    "                                  y_train_rolled_without_semantics\n",
    "                                  )\n",
    "\n",
    "RF_feature_model_predictions_without_semantics = RF_feature_model_without_semantics.predict(feature_converter_without_semantics.transform(X_valid_rolled_without_semantics)\n",
    "                                                                                  )\n",
    "\n",
    "rms_feature_without_semantics = sqrt(mean_squared_error(y_valid_rolled_without_semantics,\n",
    "                                                   RF_feature_model_predictions_without_semantics\n",
    "                                                   )\n",
    "                                )\n",
    "\n",
    "print(\"Root mean squared error on valid:\", rms_feature_without_semantics)\n",
    "print(\"Root mean squared error on valid inverse transformed from normalization:\", normalizers_without_semantics[\"OPEN\"]\n",
    "      .inverse_transform(np.array([rms_feature_without_semantics]).reshape(1, -1)))\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "RF_feature_model_predictions_without_semantics = normalizers_without_semantics['OPEN']\\\n",
    "                                .inverse_transform(np.array(RF_feature_model_predictions_without_semantics).reshape(-1, 1))\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(' ')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(RF_feature_model_predictions_flair_content, color='green', label='Predicted BMW Stock Price with flair content analysis')\n",
    "plt.plot(RF_feature_model_predictions_flair_header, color='red', label='Predicted BMW Stock Price with flair header analysis')\n",
    "plt.plot(RF_feature_model_predictions_textblob_content, color='orange', label='Predicted BMW Stock Price with textblob content analysis')\n",
    "plt.plot(RF_feature_model_predictions_textblob_header, color='blue', label='Predicted BMW Stock Price with textblob header analysis')\n",
    "plt.plot(RF_feature_model_predictions_vader_content, color='cyan', label='Predicted BMW Stock Price with vader content analysis')\n",
    "plt.plot(RF_feature_model_predictions_vader_header, color='magenta', label='Predicted BMW Stock Price with vader header analysis')\n",
    "plt.plot(RF_feature_model_predictions_without_semantics, color='yellow', label='Predicted BMW Stock Price without semantics analysis')\n",
    "plt.title('BMW Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('BMW Stock Price')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.005), borderaxespad=8)\n",
    "\n",
    "date_today = str(datetime.now().strftime(\"%Y%m%d\"))\n",
    "plt.savefig(r'C:\\Users\\victo\\Master_Thesis\\stockprice_prediction\\RandomForest_feature_model\\bmw\\minutely\\prediction_bmw_' + date_today + '.png',\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=100,\n",
    "            pad_inches=1.5)\n",
    "plt.show()\n",
    "print('Run is finished and plot is saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
